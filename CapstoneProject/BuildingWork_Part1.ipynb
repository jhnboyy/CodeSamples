{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d27438-7721-400f-a735-3890a7730120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing needed libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import time\n",
    "from json import JSONDecodeError\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.discrete.count_model import ZeroInflatedPoisson\n",
    "from personal_lib import general_functions as gf\n",
    "import re\n",
    "from typing import Optional, List, Tuple\n",
    "import googlemaps\n",
    "import os\n",
    "import math\n",
    "import pyogrio\n",
    "import geopandas as gpd\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "creds = gf.get_creds()\n",
    "google_api_key = creds[\"Google\"][\"geocoding\"]\n",
    "open_ai = creds[\"openai\"][\"FirstTestKey\"]\n",
    "gmaps = googlemaps.Client(key=google_api_key)\n",
    "\n",
    "STATE_ABBR = \"NY\"\n",
    "pd.options.display.max_columns = 500 \n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"capstone_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c1833-ac43-4433-9281-8dca4771af89",
   "metadata": {},
   "source": [
    "### NYC ENERGY DATA BUILDINGS (### Pulling in the Data Via the APi online Local Law 84) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ade96e-47f3-49a6-b4bf-3c61fe83a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are mulitple sources of yearly data. Compiling the source URLs here as well as the API urls to grab\n",
    "## privately owned buildings over 25,000 ft2 and in City-owned buildings over 10,000 ft2\n",
    "building_energy_LL84_sources={\"2022+\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/5zyy-y8am.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/NYC-Building-Energy-and-Water-Data-Disclosure-for-/5zyy-y8am/about_data\"\n",
    "                              },\n",
    "                              \"2021\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/7x5e-2fxh.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/7x5e-2fxh/about_data\"\n",
    "                              },\n",
    "                              \"2020\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/usc3-8zwd.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/usc3-8zwd/about_data\"\n",
    "                                  },\n",
    "                              \"2019\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/wcm8-aq5w.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/wcm8-aq5w/about_data\"\n",
    "                                  },\n",
    "                              \"2018\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/4tys-3tzj.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/4tys-3tzj/about_data\"\n",
    "                                   },\n",
    "                              \"2017\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/4t62-jm4m.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/4t62-jm4m/about_data\"\n",
    "                                  },\n",
    "                              \"2016\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/utpj-74fz.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/utpj-74fz/about_data\"\n",
    "                                  },\n",
    "                              \"2015\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/77q4-nkfh.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/77q4-nkfh/about_data\"\n",
    "                                  },\n",
    "                              \"2014\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/nbun-wekj.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/nbun-wekj/about_data\"\n",
    "                                  },\n",
    "                              \"2013\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/yr5p-wjer.json\",\n",
    "    \"info\":\"http://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/yr5p-wjer\"\n",
    "                                  },\n",
    "                              \"2012\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/r6ub-zhff.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/r6ub-zhff/about_data\"\n",
    "                                  },\n",
    "                              \"2011\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/k7nh-aufb.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/k7nh-aufb/about_data\"\n",
    "                                  },\n",
    "                              \"2010\":{\n",
    "    \"api\":\"https://data.cityofnewyork.us/resource/kswi-37bp.json\",\n",
    "    \"info\":\"https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/kswi-37bp/about_data\"\n",
    "                                  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b624c6b-1e3a-41a1-8b06-87c5783c5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE = 1000           \n",
    "TIMEOUT = 30\n",
    "MAX_RETRIES = 5\n",
    "BACKOFF_BASE = 1.5\n",
    "\n",
    "session = requests.Session()\n",
    "headers = {}\n",
    "\n",
    "def fetch_all_rows_1k(api_url: str, source_years: str, source_info_url: str) -> pd.DataFrame:\n",
    "    offset = 0\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        params = {\"$limit\": PAGE, \"$offset\": offset}\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                resp = session.get(api_url, params=params, headers=headers, timeout=TIMEOUT)\n",
    "                if resp.status_code in (429, 502, 503, 504):\n",
    "                    time.sleep(BACKOFF_BASE ** attempt * (0.1 * attempt))\n",
    "                    continue\n",
    "                resp.raise_for_status()\n",
    "\n",
    "                if \"json\" not in resp.headers.get(\"Content-Type\", \"\").lower():\n",
    "                    preview = resp.text[:200]\n",
    "                    raise ValueError(f\"Non-JSON response (status {resp.status_code}): {preview}\")\n",
    "\n",
    "                data_chunk = resp.json()\n",
    "                if not data_chunk:\n",
    "                    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "                # Create DataFrame\n",
    "                df = pd.DataFrame(data_chunk)\n",
    "\n",
    "                # ðŸ”‘ Add your metadata columns here\n",
    "                df[\"source_years\"] = source_years\n",
    "                df[\"source_api_url\"] = api_url\n",
    "                df[\"source_info_url\"] = source_info_url\n",
    "\n",
    "                frames.append(df)\n",
    "\n",
    "                # If less than PAGE, stop; otherwise keep paginating\n",
    "                if len(data_chunk) < PAGE:\n",
    "                    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "                offset += PAGE\n",
    "                break  # Success, go to next page\n",
    "\n",
    "            except (requests.RequestException, JSONDecodeError, ValueError) as e:\n",
    "                if attempt == MAX_RETRIES:\n",
    "                    print(f\"âš ï¸ Failed fetching {api_url} at offset {offset}: {e}\")\n",
    "                    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "                time.sleep(BACKOFF_BASE ** attempt * (0.1 * attempt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33506676-afbd-4c9b-a55e-a3e7ee0d654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_running_list = []\n",
    "for k, v in building_energy_LL84_sources.items():\n",
    "    print(f\"Fetching {k} -> {v['api']}\")\n",
    "    df = fetch_all_rows_1k(v[\"api\"], k, v[\"info\"])\n",
    "    if not df.empty:\n",
    "        agg_running_list.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: no rows returned for {k} ({v['api']}).\")\n",
    "\n",
    "nyc_building_energy = pd.concat(agg_running_list, ignore_index=True) if agg_running_list else pd.DataFrame()\n",
    "print(f\"Total rows: {len(nyc_building_energy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65963e33-526c-4710-ae8b-b5956f26442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Columns to Drop \n",
    "to_drop =[\n",
    "## Banking oriented COlumns\n",
    "'bank_branch_computer_density',\n",
    " 'bank_branch_gross_floor_area',\n",
    " 'bank_branch_gross_floor_area_ft',\n",
    " 'bank_branch_number_of',\n",
    " 'bank_branch_number_of_workers',\n",
    " 'bank_branch_percent_that',\n",
    " 'bank_branch_weekly_operating',\n",
    " 'bank_branch_worker_density',\n",
    "## College / Uni / School\n",
    "'college_university_gross',\n",
    " 'college_university_gross_floor_area_ft',\n",
    " 'college_university_number',\n",
    "    'k_12_school_computer_density',\n",
    " 'k_12_school_cooking_facilities',\n",
    " 'k_12_school_gross_floor_area',\n",
    " 'k_12_school_gross_floor_area_ft',\n",
    " 'k_12_school_high_school',\n",
    " 'k_12_school_percent_that',\n",
    " 'k_12_school_refrigeration',\n",
    " 'k_12_school_weekend_operation',\n",
    " 'laboratory_gross_floor_area_ft',\n",
    "    'library_gross_floor_area',\n",
    "# OTher\n",
    "\"automobile_dealership_gross\",\n",
    "'convenience_store_without',\n",
    "'data_center_energy_estimates_applied',\n",
    " 'data_center_gross_floor_area',\n",
    " 'data_center_gross_floor_area_ft',\n",
    " 'data_center_it_energy',\n",
    " 'data_center_it_energy_configuration',\n",
    " 'data_center_it_equipment_input_meter_kwh',\n",
    " 'data_center_it_site_energy',\n",
    " 'data_center_it_site_energy_kwh',\n",
    " 'data_center_it_source_energy_kbtu',\n",
    " 'data_center_national_median',\n",
    " 'data_center_pdu_input_meter_kwh',\n",
    " 'data_center_pdu_output_meter_kwh',\n",
    " 'data_center_ups_output_meter_kwh',\n",
    " 'enclosed_mall_gross_floor',\n",
    " 'enclosed_mall_gross_floor_area_ft',\n",
    " 'fast_food_restaurant_gross',\n",
    " 'financial_office_gross_floor',\n",
    " 'financial_office_gross_floor_area_ft',\n",
    " 'financial_office_number_of',\n",
    " 'financial_office_number_of_1',\n",
    " 'financial_office_number_of_computers',\n",
    " 'financial_office_number_of_workers_on_main_shift',\n",
    " 'financial_office_weekly',\n",
    " 'financial_office_weekly_operating_hours',\n",
    " 'fitness_center_health_club',\n",
    " 'fitness_center_health_club_gym_gross_floor_area_ft',\n",
    " 'food_sales_gross_floor_area',\n",
    " 'food_sales_gross_floor_area_ft',\n",
    " 'food_service_gross_floor',\n",
    " 'food_service_gross_floor_area_ft',\n",
    "     'worship_facility_computer',\n",
    " 'worship_facility_cooking',\n",
    " 'worship_facility_gross_floor',\n",
    " 'worship_facility_gross_floor_area_ft',\n",
    " 'worship_facility_weekly',\n",
    "    'supermarket_grocery_cooking',\n",
    " 'supermarket_grocery_gross',\n",
    " 'supermarket_grocery_gross_floor_area_ft',\n",
    " 'supermarket_grocery_number',\n",
    " 'supermarket_grocery_number_1',\n",
    " 'supermarket_grocery_number_2',\n",
    " 'supermarket_grocery_number_of_open_or_closed_refrigeration_freezer_units',\n",
    " 'supermarket_grocery_number_of_walk_in_refrigeration_freezer_units',\n",
    " 'supermarket_grocery_percent',\n",
    " 'supermarket_grocery_walk',\n",
    " 'supermarket_grocery_weekly',\n",
    " 'supermarket_grocery_worker',\n",
    " 'swimming_pool_approximate',\n",
    " 'swimming_pool_location_of',\n",
    " 'swimming_pool_months_in_use',\n",
    "     'social_meeting_hall_gross',\n",
    " 'social_meeting_hall_gross_floor_area_ft',\n",
    "     'residence_hall_dormitory',\n",
    " 'residence_hall_dormitory_1',\n",
    " 'residence_hall_dormitory_2',\n",
    " 'residence_hall_dormitory_3',\n",
    " 'residence_hall_dormitory_4',\n",
    " 'residence_hall_dormitory_gross_floor_area_ft',\n",
    " 'restaurant_gross_floor_area',\n",
    " 'restaurant_gross_floor_area_ft',\n",
    " 'restaurant_weekly_operating',\n",
    " 'restaurant_weekly_operating_hours',\n",
    " 'restaurant_worker_density',\n",
    " 'restaurant_worker_density_number_per_1_000_sq_ft',\n",
    " 'retail_store_cash_register',\n",
    " 'retail_store_computer_density',\n",
    " 'retail_store_exterior_entrance',\n",
    " 'retail_store_gross_floor',\n",
    " 'retail_store_gross_floor_area_ft',\n",
    " 'retail_store_number_of_open',\n",
    " 'retail_store_number_of_open_or_closed_refrigeration_freezer_units',\n",
    " 'retail_store_number_of_walk',\n",
    " 'retail_store_number_of_walk_in_refrigeration_freezer_units',\n",
    " 'retail_store_open_or_closed',\n",
    " 'retail_store_percent_that',\n",
    " 'retail_store_walk_in',\n",
    " 'retail_store_weekly_operating',\n",
    " 'retail_store_worker_density',\n",
    " 'self_storage_facility_gross',\n",
    " 'self_storage_facility_gross_floor_area_ft',\n",
    " 'senior_care_community_average',\n",
    " 'senior_care_community_gross',\n",
    " 'senior_care_community_maximum',\n",
    " 'senior_care_community_number',\n",
    " 'senior_care_community_number_1',\n",
    " 'senior_care_community_number_2',\n",
    " 'senior_care_community_number_3',\n",
    " 'senior_care_community_number_4',\n",
    " 'senior_care_community_number_5',\n",
    " 'senior_care_community_number_6',\n",
    " 'senior_care_community_percent',\n",
    " 'senior_living_community_gross_floor_area_ft',\n",
    " 'senior_living_community_living_unit_density_number_per_1_000_sq_ft',\n",
    "    'movie_theater_gross_floor_area_ft',\n",
    "    'non_refrigerated_warehouse',\n",
    " 'non_refrigerated_warehouse_1',\n",
    " 'non_refrigerated_warehouse_2',\n",
    " 'non_refrigerated_warehouse_3',\n",
    " 'non_refrigerated_warehouse_4',\n",
    " 'non_refrigerated_warehouse_5',\n",
    " 'non_refrigerated_warehouse_gross_floor_area_ft',\n",
    "     'parking_completely_enclosed',\n",
    " 'parking_completely_enclosed_parking_garage_size_ft',\n",
    " 'parking_gross_floor_area',\n",
    " 'parking_gross_floor_area_ft',\n",
    " 'parking_open_parking_lot',\n",
    " 'parking_open_parking_lot_size_ft',\n",
    " 'parking_partially_enclosed',\n",
    " 'parking_partially_enclosed_parking_garage_size_ft',\n",
    "     'strip_mall_gross_floor_area',\n",
    "## Hostpial / Hotel\n",
    "    'hospital_general_medical',\n",
    " 'hospital_general_medical_1',\n",
    " 'hospital_general_medical_10',\n",
    " 'hospital_general_medical_11',\n",
    " 'hospital_general_medical_12',\n",
    " 'hospital_general_medical_13',\n",
    " 'hospital_general_medical_14',\n",
    " 'hospital_general_medical_15',\n",
    " 'hospital_general_medical_16',\n",
    " 'hospital_general_medical_17',\n",
    " 'hospital_general_medical_2',\n",
    " 'hospital_general_medical_3',\n",
    " 'hospital_general_medical_4',\n",
    " 'hospital_general_medical_5',\n",
    " 'hospital_general_medical_6',\n",
    " 'hospital_general_medical_7',\n",
    " 'hospital_general_medical_8',\n",
    " 'hospital_general_medical_9',\n",
    " 'hotel_amount_of_laundry',\n",
    " 'hotel_cooking_facilities',\n",
    " 'hotel_full_service_spa_floor',\n",
    " 'hotel_gross_floor_area_ft',\n",
    " 'hotel_gym_fitness_center',\n",
    " 'hotel_gym_fitness_center_floor_area_ft',\n",
    " 'hotel_number_of_rooms',\n",
    " 'hotel_percent_that_can_be',\n",
    " 'hotel_room_density_number',\n",
    " 'hotel_type_of_laundry_facility',\n",
    " 'hotel_worker_density_number',\n",
    " 'urgent_care_clinic_other',\n",
    " 'urgent_care_clinic_other_outpatient_gross_floor_area_ft',\n",
    "    'mailing_center_post_office_gross_floor_area_ft',\n",
    " 'manufacturing_industrial_plant_gross_floor_area_ft',\n",
    " 'medical_office_gross_floor',\n",
    " 'medical_office_gross_floor_area_ft',\n",
    " 'medical_office_mri_machine',\n",
    " 'medical_office_number_of',\n",
    " 'medical_office_number_of_1',\n",
    " 'medical_office_number_of_computers',\n",
    " 'medical_office_number_of_mri_machines',\n",
    " 'medical_office_number_of_workers_on_main_shift',\n",
    " 'medical_office_percent_that',\n",
    " 'medical_office_percent_that_1',\n",
    " 'medical_office_percent_that_can_be_cooled',\n",
    " 'medical_office_percent_that_can_be_heated',\n",
    " 'medical_office_weekly',\n",
    " 'medical_office_weekly_operating_hours',\n",
    "    'museum_gross_floor_area_ft',\n",
    "    'office_computer_density_number',\n",
    " 'office_gross_floor_area_ft',\n",
    " 'office_number_of_computers',\n",
    " 'office_number_of_workers',\n",
    " 'office_number_of_workers_on_main_shift',\n",
    " 'office_percent_that_can_be',\n",
    " 'office_percent_that_can_be_1',\n",
    " 'office_percent_that_can_be_cooled',\n",
    " 'office_percent_that_can_be_heated',\n",
    " 'office_weekly_operating_hours',\n",
    " 'office_worker_density_number',\n",
    " 'office_worker_density_number_per_1_000_sq_ft',\n",
    "     'adult_education_gross_floor',\n",
    " 'adult_education_gross_floor_area_ft',\n",
    "    'data_center_ups_output_meter',\n",
    " 'data_center_pdu_input_meter',\n",
    " 'data_center_pdu_output_meter',\n",
    " 'data_center_it_equipment',\n",
    " 'data_center_it_site_energy',\n",
    " 'data_center_it_source_energy',\n",
    " 'data_center_pue',\n",
    " 'data_center_national_median',\n",
    " 'data_center_gross_floor_area',\n",
    " 'data_center_ups_system',\n",
    " 'data_center_it_energy',\n",
    " 'data_center_cooling_equipment',\n",
    "'supermarkets_grocery_gross',\n",
    " 'supermarkets_grocery_number',\n",
    " 'supermarkets_grocery_number_1',\n",
    " 'supermarkets_grocery_number_2',\n",
    " 'supermarkets_grocery_percent',\n",
    " 'supermarkets_grocery_presence',\n",
    " 'supermarkets_grocery_walk',\n",
    " 'supermarkets_grocery_weekly',\n",
    " 'supermarkets_grocery_workers',\n",
    "'house_of_worship_gross_floor',\n",
    " 'house_of_worship_pc_density',\n",
    " 'house_of_worship_weekly',\n",
    " 'house_of_worship_presence',\n",
    "'residence_halls_dormitories',\n",
    "'medical_office_percent_cooled',\n",
    " 'residence_halls_dormitories_1',\n",
    " 'residence_halls_dormitories_2',\n",
    " 'residence_halls_dormitories_3',\n",
    " 'residence_halls_dormitories_4',\n",
    "'hotel_onsite_laundry_short',\n",
    " 'warehouse_unrefrigerated',\n",
    " 'warehouse_unrefrigerated_1',\n",
    " 'warehouse_unrefrigerated_2',\n",
    " 'warehouse_unrefrigerated_3',\n",
    " 'warehouse_unrefrigerated_4',\n",
    " 'warehouse_unrefrigerated_5',\n",
    " 'warehouse_unrefrigerated_6',\n",
    " 'hospital_gross_floor_area',\n",
    " 'hospital_laboratory_y_1_n',\n",
    " 'hospital_laundry_facility',\n",
    " 'hospital_maximum_number_of',\n",
    " 'hospital_number_of_buildings',\n",
    " 'warehouse_refrigerated_gross',\n",
    " 'warehouse_refrigerated_weekly',\n",
    " 'warehouse_refrigerated_workers',\n",
    " 'hospital_number_of_licensed',\n",
    " 'multifamily_home_dishwashers',\n",
    " 'fuel_oil_2_use_kbtu',\n",
    " 'fuel_oil_4_use_kbtu',\n",
    " 'diesel_2_use_kbtu',\n",
    " 'district_steam_use_kbtu',\n",
    " 'natural_gas_use_kbtu',\n",
    "'natural_gas_use_therms',\n",
    " 'difference_from_national',\n",
    " 'difference_from_national_1',\n",
    " 'current_direct_ghg_emissions',\n",
    " 'current_indirect_ghg_emissions',\n",
    "'office_gross_floor_area_sq',\n",
    " 'office_office_air_conditioned',\n",
    " 'office_pc_density',\n",
    " 'office_workers_density',\n",
    " 'parking_enclosed_floor_area',\n",
    " 'parking_non_enclosed_floor',\n",
    " 'parking_open_floor_area_w',\n",
    " 'parking_weekly_hours_of_access',\n",
    " 'retail_gross_floor_area_sq',\n",
    " 'retail_cash_register_density',\n",
    " 'retail_exterior_entrance',\n",
    " 'retail_workers_density',\n",
    " 'retail_weekly_operating_hours',\n",
    " 'retail_walk_in_refrig_density',\n",
    " 'retail_percent_cooled',\n",
    " 'retail_pc_density',\n",
    " 'retail_open_closed_refrig',\n",
    " 'retail_number_of_walk_in',\n",
    " 'retail_number_of_open_or',\n",
    "'data_center_annual_it_energy',\n",
    "'other_space_type_name',\n",
    " 'other_gross_floor_area_sq',\n",
    " 'other_number_of_pcs',\n",
    " 'other_workers_on_main_shift',\n",
    " 'hotel_gross_floor_area_sq',\n",
    " 'hotel_room_density',\n",
    " 'hotel_percent_cooled',\n",
    " 'hotel_presence_of_cooking',\n",
    " 'hotel_workers_density',\n",
    " 'bank_financial_institution',\n",
    " 'bank_financial_institution_1',\n",
    " 'bank_financial_institution_2',\n",
    " 'bank_financial_institution_3',\n",
    " 'bank_financial_institution_4',\n",
    " 'bank_financial_institution_5',\n",
    " 'k_12_school_high_school_y',\n",
    " 'k_12_school_open_weekends',\n",
    " 'k_12_school_pc_density',\n",
    " 'k_12_school_percent_cooled',\n",
    " 'k_12_school_walk_in_refrig',\n",
    " 'k_12_school_presence_of',\n",
    " 'swimming_pool_size',\n",
    " 'swimming_pool_indoor_outdoor',\n",
    " 'hotel_floor_area_of_full',\n",
    " 'hotel_floor_area_of_gym',\n",
    " 'hotel_average_occupancy',\n",
    " 'hotel_quantity_of_laundry',\n",
    "'number_of_staffed_beds',\n",
    " 'number_of_mri_machines',\n",
    " 'senior_care_facility_average',\n",
    " 'senior_care_facility_gross',\n",
    " 'senior_care_facility_number',\n",
    " 'senior_care_facility_number_1',\n",
    " 'senior_care_facility_number_2',\n",
    " 'senior_care_facility_number_3',\n",
    " 'senior_care_facility_number_4',\n",
    " 'senior_care_facility_percent',\n",
    " 'senior_care_facility_total',\n",
    " 'senior_care_facility_total_1',\n",
    " 'senior_care_facility_workers',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff4bc9-e3ea-45c5-9213-ccac17d854aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunksize = 100000  \n",
    "filtered_chunks = []\n",
    "for chunk in pd.read_csv(\"nyc_dob_energy_2010_2024.csv\", chunksize=chunksize, usecols=lambda col: col not in to_drop):\n",
    "    ## Using nulls instead of \"not Available\".\n",
    "        ## Using nulls instead of \"not Available\".\n",
    "    chunk = chunk.replace(\"Not Available\",np.nan)\n",
    "    # Convert year_ending to datetime\n",
    "    chunk[\"year_ending\"] = pd.to_datetime(chunk[\"year_ending\"], errors=\"coerce\")\n",
    "    # Optional: extract just the year if thatâ€™s all you need\n",
    "    chunk[\"year_ending_year\"] = chunk[\"year_ending\"].dt.year\n",
    "    # print(chunk[\"year_ending_year\"].unique())\n",
    "    \n",
    "    ### Property Types\n",
    "    chunk[\"primary_property_type\"] = chunk[\"primary_property_type\"].combine_first(chunk[\"primary_property_type_epa\"])\n",
    "    chunk[\"primary_property_type_self\"] = chunk[\"primary_property_type_self\"].combine_first(chunk[\"primary_property_type_self_selected\"])\n",
    "    chunk = chunk.drop(columns=[\"primary_property_type_self_selected\",\"primary_property_type_epa\"])\n",
    "\n",
    "\n",
    "    ## Beginning of Limiting to Multifamily Homes (Step 1)\n",
    "    residential_chunk = chunk[(((chunk[\"list_of_all_property_use\"].isin([i for i in chunk[\"list_of_all_property_use\"].unique() if 'Multifamily Housing' in str(i)]))\n",
    "            |(chunk[\"list_of_all_property_use\"].isnull()))\n",
    "           &((chunk[\"primary_property_type_self\"]==\"Multifamily Housing\")|(chunk[\"primary_property_type_self\"].isnull()))\n",
    "           &((chunk[\"primary_property_type\"]==\"Multifamily Housing\")|(chunk[\"primary_property_type\"].isnull())))]\n",
    "    \n",
    "    ## Second Step is limiting to NON MIXED USE, so ONLY MultiFamily Residences\n",
    "    residential_chunk2 = residential_chunk[((residential_chunk['list_of_all_property_use'].isnull())\n",
    "                                            | residential_chunk['list_of_all_property_use'].apply(\n",
    "                                                lambda x: len(x.split(\",\")) == 1 if isinstance(x, str) else False\n",
    "                                            ))]\n",
    "\n",
    "\n",
    "    ## Limtiing to the Metered Areas for whole property or whole building\n",
    "    residential_chunk3 = residential_chunk2[((residential_chunk2[\"metered_areas_energy\"].isin(['Whole Building', \"Whole Property\"]))\n",
    "                                             |(residential_chunk2[\"metered_areas_energy\"].isnull()))]\n",
    "\n",
    "    residential_chunk3 = residential_chunk3[((residential_chunk3[\"construction_status\"]=='Existing')|(residential_chunk3[\"construction_status\"].isnull()))]\n",
    "    \n",
    "    ## Stand Alone Properties\n",
    "    residential_chunk3 = residential_chunk3[((residential_chunk3[\"parent_property_id\"].isnull())\n",
    "                                             |(residential_chunk3[\"parent_property_id\"]=='Not Applicable: Standalone Property'))]\n",
    "    ## Dropping what i already filtered on\n",
    "    residential_chunk3 = residential_chunk3.drop(columns=[\"primary_property_type_self\",\"primary_property_type\",\n",
    "                                                          \"national_median_reference\", \"list_of_all_property_use\",\n",
    "                                                          \"largest_property_use_type\",\"construction_status\",\n",
    "                                                          \"parent_property_id\",\"parent_property_name\"])\n",
    "    \n",
    "    # ## Fruther Limiting to those that are under 10 Stories. \n",
    "    # residential_chunk2 = residential_chunk2.replace(\"Not Available\",np.nan).dropna(how='all',axis=1)\n",
    "    # residential_chunk2 = residential_chunk2.dropna(how='all',axis=1)\n",
    "\n",
    "    filtered_chunks.append(residential_chunk3)\n",
    "\n",
    "residential3 = pd.concat(filtered_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0cddab-964e-4607-b7a8-3fbb5bb14eb0",
   "metadata": {},
   "source": [
    "### Fixing Via Self Joins and Otherwise for Earlier years in data without Property ID / BBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e21433-6e43-47ce-88a1-9d036c509568",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finalize the truth set year column \n",
    "residential3[\"year_ending_year\"] = residential3[\"year_ending_year\"].combine_first(residential3[\"source_years\"]).astype(int)\n",
    "### Consolidating those columns that contain potential ID numbers\n",
    "residential3[\"bbl\"] = residential3[\"nyc_borough_block_and_lot\"].combine_first(residential3['nyc_borough_block_and_lot_bbl']).combine_first(residential3[\"bbl10\"]).combine_first(residential3[\"bbl\"])\n",
    "residential3.loc[residential3[\"bbl\"].notna(), \"bbl\"] = residential3.loc[residential3[\"bbl\"].notna(), \"bbl\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True).str.replace(r\"-\", \"\", regex=False).str.replace(r\"/\", \"\", regex=False).str.replace(r\".\", \"\", regex=False)\n",
    "residential3[\"postcode\"] = residential3[\"postcode\"].combine_first(residential3[\"postal_code\"])\n",
    "residential3['postcode'] = residential3['postcode'].apply(lambda x : str(x).replace(\",0\",'') if x else x)\n",
    "residential3 = residential3.drop(columns=[\"postal_code\",\"nyc_borough_block_and_lot\",'nyc_borough_block_and_lot_bbl',\"bbl10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503145e8-0933-4995-9248-95f9419e53d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Checking the row numbers and Property Id vals\n",
    "residential3.groupby([\"year_ending_year\"]).agg({\"property_id\":[\"nunique\",\"count\"],\"source_api_url\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bc36e-4c0d-4071-83bc-27dd71000ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attempting to self-enrich data for property ids\n",
    "null_property_ids = residential3[residential3[\"property_id\"].isnull()]\n",
    "print(null_property_ids.shape)\n",
    "property_ids = residential3[~residential3[\"property_id\"].isnull()]\n",
    "print(property_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c90db-987a-4f12-b85a-d15ca8e00fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### immediate Crosswalk for BBL and Property IDs Merging \n",
    "cw_join = property_ids[[\"property_id\",\"bbl\"]][~property_ids[\"bbl\"].isnull()].drop_duplicates()\n",
    "null_property_ids_enr1 = null_property_ids[[c for c in null_property_ids.columns if c !=\"property_id\"]].merge(cw_join,how='left',on=[\"bbl\"])\n",
    "unresolved = null_property_ids_enr1[null_property_ids_enr1[\"property_id\"].isnull()]\n",
    "resolved = null_property_ids_enr1[~null_property_ids_enr1[\"property_id\"].isnull()]\n",
    "property_ids = pd.concat([property_ids,resolved]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7115b5a-1b58-46be-8eca-7277edab270d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Taking a look\n",
    "property_ids.groupby([\"property_id\"]).agg({\"bbl\":[\"nunique\",\"count\"],\"source_api_url\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441cecd-0f58-4a8c-a9fa-05c2a3bf812b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "property_ids.groupby([\"year_ending_year\"]).agg({\"property_id\":[\"nunique\",\"count\"],\"source_api_url\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70327a2d-b299-4d23-b876-ef08ee6078cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Resolving Mulitple BBLS per property Id\n",
    "test = property_ids[[\"property_id\", \"bbl\", \"address_1\", \"city\",\"latitude\", \"longitude\", \"postcode\", \"year_ending_year\"]].drop_duplicates()\n",
    "test2 = (test.groupby(\"property_id\").agg(bbl_nunique=(\"bbl\", \"nunique\"),address_nunique=(\"address_1\", \"nunique\"),\n",
    "                                         year_min=(\"year_ending_year\", \"min\"),year_max=(\"year_ending_year\", \"max\"),).reset_index())\n",
    "to_fix = test2[test2[\"bbl_nunique\"] > 1]\n",
    "to_fix = to_fix[((to_fix[\"year_min\"]==2010)\n",
    "        &(to_fix[\"year_max\"]>=2017))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111db83-be38-415c-9049-f1548f7d5466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, r in to_fix.iterrows():\n",
    "    print(r[\"property_id\"])\n",
    "    temp_df = property_ids[[\"property_id\",\"bbl\",\"year_ending\",\"address_1\",\"city\",\"postcode\"]][property_ids[\"property_id\"]==r['property_id']]\n",
    "    print(temp_df['bbl'].unique())\n",
    "    #Most frequent (mode) BBL for this property_id\n",
    "    mode_bbl = temp_df['bbl'].mode(dropna=True).iloc[0]\n",
    "    if \";\" in mode_bbl:\n",
    "        continue\n",
    "    else:\n",
    "        print(temp_df.shape)\n",
    "        # Write that BBL back onto ALL rows for this property_id\n",
    "        property_ids.loc[property_ids[\"property_id\"] == r['property_id'], \"bbl\"] = mode_bbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58072ebc-9c07-4dd6-98db-f7efa06cd7de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Putting together Processed Results So Far \n",
    "## Keepgin the unresolved with BBL values, dropping those that are null because BBL and Property ID are null.\n",
    "print(unresolved[unresolved[\"bbl\"].isnull()].shape)\n",
    "print(unresolved[~unresolved[\"bbl\"].isnull()].shape)\n",
    "# test = pd.concat()\n",
    "keep = unresolved[~unresolved[\"bbl\"].isnull()]\n",
    "keep[\"resolved_flag\"]=\"UNRESOLVED\"\n",
    "property_ids[\"resolved_flag\"]=\"RESOLVED\"\n",
    "residential3_refined= pd.concat([property_ids,keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37040f0e-cc5e-4c73-b3f7-714a0ee3d266",
   "metadata": {},
   "source": [
    "#### Limiting by Years and for those Buildigns that have been in the Data for the years we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af33fe3-20ff-4b21-a425-bffb324ed033",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_2010_property_ids = residential3_refined[((residential3_refined[\"year_ending_year\"].isin([2010,2011]))\n",
    "                                                    &(~residential3_refined[\"property_id\"].isnull()))][\"property_id\"].unique()\n",
    "buildings_2010_bbls = residential3_refined[((residential3_refined[\"year_ending_year\"].isin([2010,2011]))\n",
    "                                            &(~residential3_refined[\"bbl\"].isnull()))][\"bbl\"].unique()\n",
    "print(\"CheckingGrouping - Prop ID\")\n",
    "print(residential3_refined[residential3_refined['property_id'].isin(buildings_2010_property_ids)].groupby(['year_ending_year']).agg({\"address_1\":\"count\"}).reset_index())\n",
    "print(\"CheckingGrouping - BBL\")\n",
    "print(residential3_refined[residential3_refined['bbl'].isin(buildings_2010_bbls)].groupby(['year_ending_year']).agg({\"address_1\":\"count\"}).reset_index())\n",
    "### Limtiing to 2017 and before \n",
    "residential3_refined_b4_2018 = residential3_refined[residential3_refined[\"year_ending_year\"]<2018]\n",
    "print(\"YEARS\")\n",
    "print(residential3_refined_b4_2018['year_ending_year'].unique())\n",
    "residential3_2010_2017 = residential3_refined_b4_2018[((residential3_refined_b4_2018['property_id'].isin(buildings_2010_property_ids))\n",
    "                                                       |(residential3_refined_b4_2018['bbl'].isin(buildings_2010_bbls)))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e923fb-bd25-4da0-b210-300b72b97ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building Info Only \n",
    "building_info =residential3_2010_2017[[\"property_id\",'bbl',\"address_1\",\"address_2\",\"city\",\"postcode\",'county','borough',\"latitude\",\"longitude\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fafa6-84dc-441f-b243-b2b0b3324225",
   "metadata": {},
   "source": [
    "#### Cleaning Address, Prepping for Geocoding where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e3992-177a-4e32-b4ba-535d9c312695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC biasing bounds (rough NYC box)\n",
    "NYC_BOUNDS = {\n",
    "    \"southwest\": {\"lat\": 40.477399, \"lng\": -74.259090},\n",
    "    \"northeast\": {\"lat\": 40.917577, \"lng\": -73.700272},\n",
    "}\n",
    "\n",
    "# Common misspellings / normalizations seen in your data\n",
    "COMMON_FIXES = {\n",
    "    r\"\\bWASHIGNTON\\b\": \"WASHINGTON\",\n",
    "    r\"\\bWASHNGTN\\b\": \"WASHINGTON\",\n",
    "    r\"\\bALBERMALE\\b\": \"ALBEMARLE\",\n",
    "    r\"\\bALBERMALE\\b\": \"ALBEMARLE\",\n",
    "    r\"\\bAMSETRDAM\\b\": \"AMSTERDAM\",\n",
    "    r\"\\bKINGBRIDGE\\b\": \"KINGSBRIDGE\",\n",
    "    r\"\\bOVINGTON\\b\": \"OVINGTON\",   # keep as is (looks fine)\n",
    "    r\"\\bNAGLE HOUSE\\b\": \"NAGLE AVE\",  # likely street not building name\n",
    "    r\"\\bNAGLE\\b\": \"NAGLE\",  # pass-through\n",
    "    r\"\\bST\\s*NICOLAS\\b\": \"ST NICHOLAS\",\n",
    "    r\"\\bOILVER\\b\": \"OLIVER\",\n",
    "    r\"\\bRIVER SIDE\\b\": \"RIVERSIDE\",\n",
    "    r\"\\bSTEET\\b\": \"STREET\",\n",
    "    r\"\\bSREET\\b\": \"STREET\",\n",
    "    r\"\\bFOR WASHINGTON\\b\": \"FORT WASHINGTON\",\n",
    "    r\"\\bFT\\.?\\b\": \"FORT\",\n",
    "    r\"\\bBIVONA\\b\": \"BIVONA\",  # leave\n",
    "    r\"\\bCLARKE\\b\": \"CLARKE\",  # leave\n",
    "    r\"\\bCAROLL\\b\": \"CARROLL\",\n",
    "    r\"\\bCLAFFIN\\b\": \"CLAFLIN\",\n",
    "    r\"\\bDEKALB\\b\": \"DEKALB\",\n",
    "    r\"\\bLAFAYETTE\\b\": \"LAFAYETTE\",\n",
    "    r\"\\bW\\s*MOSHOLU\\b\": \"W MOSHOLU\",\n",
    "    r\"\\bE\\s*MOSHOLU\\b\": \"E MOSHOLU\",\n",
    "    r\"\\bMOSHOLU\\b\": \"MOSHOLU\",\n",
    "    r\"\\bPARSON\\b\": \"PARSONS\",\n",
    "    r\"\\bPARSONS BLVD?\\b\": \"PARSONS BLVD\",\n",
    "}\n",
    "\n",
    "# Street type vocabulary to help \"best-guess\" if clearly missing\n",
    "STREET_TYPES = [\"ST\", \"AVE\", \"RD\", \"BLVD\", \"PL\", \"LN\", \"DR\", \"CT\", \"PKWY\", \"TER\", \"PLZ\"]\n",
    "\n",
    "def _norm_ws(s: Optional[str]) -> Optional[str]:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return None\n",
    "    s = str(s)\n",
    "    # HTML escapes commonly seen\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s if s else None\n",
    "\n",
    "def _apply_common_fixes(u: str) -> str:\n",
    "    for pat, repl in COMMON_FIXES.items():\n",
    "        u = re.sub(pat, repl, u, flags=re.IGNORECASE)\n",
    "    return u\n",
    "\n",
    "def _std_unit(s: Optional[str]) -> Optional[str]:\n",
    "    u = _norm_ws(s)\n",
    "    if not u:\n",
    "        return None\n",
    "    u = u.upper().replace(\"#\", \"\").strip()\n",
    "    if not u:\n",
    "        return None\n",
    "    u = re.sub(r\"^(APARTMENT|APT\\.?)\\s*\", \"APT \", u)\n",
    "    u = re.sub(r\"^(SUITE|STE\\.?)\\s*\", \"STE \", u)\n",
    "    u = re.sub(r\"^(FLOOR|FL\\.?)\\s*\", \"FL \", u)\n",
    "    if not re.match(r\"^(APT|STE|FL)\\b\", u):  # bare \"5B\" â†’ \"APT 5B\"\n",
    "        u = f\"APT {u}\"\n",
    "    return u\n",
    "\n",
    "def _std_street(u: Optional[str]) -> Optional[str]:\n",
    "    u = _norm_ws(u)\n",
    "    if not u:\n",
    "        return None\n",
    "    u = _apply_common_fixes(u.upper())\n",
    "\n",
    "    repl = {\n",
    "        \" STREET\": \" ST\",\n",
    "        \" AVENUE\": \" AVE\",\n",
    "        \" ROAD\": \" RD\",\n",
    "        \" BOULEVARD\": \" BLVD\",\n",
    "        \" PLACE\": \" PL\",\n",
    "        \" LANE\": \" LN\",\n",
    "        \" DRIVE\": \" DR\",\n",
    "        \" COURT\": \" CT\",\n",
    "        \" PARKWAY\": \" PKWY\",\n",
    "        \" TERRACE\": \" TER\",\n",
    "        \" PLAZA\": \" PLZ\",\n",
    "    }\n",
    "    for k, v in repl.items():\n",
    "        u = re.sub(k + r\"\\b\", v, u)\n",
    "\n",
    "    # Normalize ordinals and add missing \"TH/ST/ND/RD\" on bare numbers like \"E 48\" or \"West104\"\n",
    "    u = re.sub(r\"\\b(\\d+)\\s*(ST|ND|RD|TH)\\b\", lambda m: f\"{int(m.group(1))}{m.group(2)}\", u)\n",
    "    u = re.sub(r\"\\b(WEST|W|EAST|E)\\s*(\\d{1,3})(?=\\b)\", r\"\\1 \\2\", u)  # ensure space\n",
    "    def add_ordinal(m):\n",
    "        n = int(m.group(2))\n",
    "        suf = \"TH\"\n",
    "        if n % 10 == 1 and n % 100 != 11: suf = \"ST\"\n",
    "        elif n % 10 == 2 and n % 100 != 12: suf = \"ND\"\n",
    "        elif n % 10 == 3 and n % 100 != 13: suf = \"RD\"\n",
    "        return f\"{m.group(1)} {n}{suf}\"\n",
    "    u = re.sub(r\"\\b(WEST|W|EAST|E)\\s+(\\d{1,3})\\b(?!\\s*(ST|AVE|RD|BLVD|PL|LN|DR|CT|PKWY|TER|PLZ))\", add_ordinal, u)\n",
    "\n",
    "    # Collapse multiple addresses to last one here (we also split earlier in the pipeline)\n",
    "    if \"/\" in u:\n",
    "        u = u.split(\"/\")[-1].strip()\n",
    "\n",
    "    # Queens: hyphenated house numbers like \"41-07 42ND ST\" are valid; leave them as-is\n",
    "    # Best-guess add a street type if line is like \"308 WEST 104TH\" (no type) or \"160 E 48TH\"\n",
    "    if re.search(r\"\\b(WEST|W|EAST|E|NORTH|N|SOUTH|S)\\b\", u) and re.search(r\"\\b\\d{1,3}(ST|ND|RD|TH)\\b\", u) and not re.search(r\"\\b(ST|AVE|RD|BLVD|PL|LN|DR|CT|PKWY|TER|PLZ)\\b\", u):\n",
    "        u = u + \" ST\"\n",
    "\n",
    "    # Convert key phrases\n",
    "    u = re.sub(r\"\\bFT\\b\", \"FORT\", u)\n",
    "\n",
    "    return u.title()\n",
    "\n",
    "def _borough_from_fields(borough, county, city) -> Optional[str]:\n",
    "    vals = \" \".join([str(x) for x in [borough, county, city] if pd.notna(x)]).upper()\n",
    "    if any(x in vals for x in [\"MANHATTAN\", \"NEW YORK\", \"NY COUNTY\"]): return \"MANHATTAN\"\n",
    "    if \"BRONX\" in vals: return \"BRONX\"\n",
    "    if any(x in vals for x in [\"BROOKLYN\", \"KINGS\"]): return \"BROOKLYN\"\n",
    "    if \"QUEENS\" in vals: return \"QUEENS\"\n",
    "    if any(x in vals for x in [\"STATEN\", \"RICHMOND\"]): return \"STATEN ISLAND\"\n",
    "    return None\n",
    "\n",
    "def _usps_city_from_borough(city, borough_norm) -> Optional[str]:\n",
    "    c = (_norm_ws(city) or \"\").upper()\n",
    "    if borough_norm == \"MANHATTAN\": return \"New York\"\n",
    "    if borough_norm == \"BRONX\": return \"Bronx\"\n",
    "    if borough_norm == \"BROOKLYN\": return \"Brooklyn\"\n",
    "    if borough_norm == \"QUEENS\":\n",
    "        # USPS accepts neighborhoods, but \"Queens\" is safest default\n",
    "        return \"Queens\" if c in [\"\", \"QUEENS\", \"NAN\"] else c.title()\n",
    "    if borough_norm == \"STATEN ISLAND\": return \"Staten Island\"\n",
    "    return c.title() if c else None\n",
    "\n",
    "def _std_zip5(z):\n",
    "    if z is None or (isinstance(z, float) and pd.isna(z)):\n",
    "        return None\n",
    "    s = re.sub(r\"\\D\", \"\", str(z))\n",
    "    return s[:5] if len(s) >= 5 else None\n",
    "\n",
    "# ---------- NEW: address candidate generator ----------\n",
    "\n",
    "SEP_PATTERN = re.compile(r\"\\s*(?:;|,|/|&{1,2}| and | AND |\\bETAL\\b|\\bet al\\b|\\(|\\)|\\bAKA\\b|\\bA\\/K\\/A\\b)\\s*\", flags=re.IGNORECASE)\n",
    "\n",
    "def _is_pobox(s: str) -> bool:\n",
    "    return bool(re.search(r\"\\bP\\.?\\s*O\\.?\\s*BOX\\b\", s, flags=re.IGNORECASE))\n",
    "\n",
    "def _has_street_name(u: str) -> bool:\n",
    "    # needs something besides just a house number or zeros\n",
    "    return bool(re.search(r\"\\b[A-Z][A-Z]+\\b\", u)) and not re.fullmatch(r\"\\d+(-\\d+)?\", re.sub(r\"[^0-9\\-]\", \"\", u or \"\"))\n",
    "\n",
    "def _expand_number_ranges(u: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    '2078-84-90 Morris Ave' -> ['2078 Morris Ave','2084 Morris Ave','2090 Morris Ave']\n",
    "    '1051-1057-1061-1065 Boston Rd' -> same expansion\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\b(\\d{1,6}(?:-\\d{1,6})+)\\s+([A-Z].+)$\", u)\n",
    "    if not m:\n",
    "        return [u]\n",
    "    block = m.group(1)\n",
    "    tail = m.group(2)\n",
    "    parts = [p for p in re.split(r\"-\", block) if p]\n",
    "    base = parts[0]\n",
    "    expanded = []\n",
    "    for p in parts:\n",
    "        if len(p) < len(base):  # \"2078-84\" -> fill high-order digits from base\n",
    "            p = base[:len(base)-len(p)] + p\n",
    "        expanded.append(f\"{p} {tail}\")\n",
    "    return expanded\n",
    "\n",
    "def _split_multi(s: str) -> List[str]:\n",
    "    s = SEP_PATTERN.sub(\" | \", s)  # unify to pipe, then split\n",
    "    tokens = [t.strip(\" .\") for t in s.split(\"|\") if _norm_ws(t)]\n",
    "    # Remove address-like fragments that are obviously units/notes only\n",
    "    return [t for t in tokens if t and not re.match(r\"^(APT|STE|FL|B#\\d+)\\b\", t, flags=re.IGNORECASE)]\n",
    "\n",
    "def build_candidates(row) -> List[Tuple[str, dict, str]]:\n",
    "    \"\"\"\n",
    "    Returns list of (query, components, reason_tag)\n",
    "    \"\"\"\n",
    "    a1 = _std_street(row.get(\"address_line1_clean\"))\n",
    "    a2 = _std_unit(row.get(\"address_line2_clean\"))\n",
    "    borough = _borough_from_fields(row.get(\"borough_clean\"), row.get(\"county\"), row.get(\"city_clean\"))\n",
    "    city = _usps_city_from_borough(row.get(\"city_clean\"), borough)\n",
    "    zip5 = _std_zip5(row.get(\"zip\"))\n",
    "    state = STATE_ABBR\n",
    "\n",
    "    raw = _norm_ws(row.get(\"geocode_key\") or row.get(\"address_1\") or \"\")\n",
    "    raw = _apply_common_fixes(raw.upper()) if raw else \"\"\n",
    "    raw_parts = _split_multi(raw) if raw else []\n",
    "\n",
    "    # Build an address seed from clean fields\n",
    "    seeds = []\n",
    "    if a1:\n",
    "        seeds.append(a1)\n",
    "    seeds.extend(_expand_number_ranges(x) for x in raw_parts)\n",
    "    seeds = [y for x in seeds for y in (x if isinstance(x, list) else [x])]\n",
    "    seeds = [s for s in seeds if s] or ([raw.title()] if raw else [])\n",
    "\n",
    "    candidates = []\n",
    "    for s in seeds:\n",
    "        s_norm = _std_street(s)\n",
    "        if not s_norm:\n",
    "            continue\n",
    "        if _is_pobox(s_norm):\n",
    "            candidates.append((None, {}, \"POBOX\"))\n",
    "            continue\n",
    "        if not _has_street_name(s_norm):\n",
    "            candidates.append((None, {}, \"MISSING_STREET\"))\n",
    "            continue\n",
    "\n",
    "        line = s_norm\n",
    "        parts = [line]\n",
    "        # Append unit if it looks like an apartment (rarely helpful for geocode but OK)\n",
    "        # if a2: parts.append(a2)\n",
    "\n",
    "        locality = city or (borough.title() if borough else None)\n",
    "        tail = \", \".join([p for p in [locality, state, zip5] if p])\n",
    "        query = f\"{' '.join(parts)}, {tail}\" if tail else \" \".join(parts)\n",
    "\n",
    "        comps = {\"administrative_area\": state}\n",
    "        if locality: comps[\"locality\"] = locality\n",
    "\n",
    "        candidates.append((query, comps, \"OK\"))\n",
    "    # Deduplicate, keep order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for q, c, r in candidates:\n",
    "        key = (q or r, tuple(sorted(c.items())))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append((q, c, r))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81b178-6905-423c-b078-337b253cbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned drafts\n",
    "building_info[\"address_line1_clean\"] = building_info[\"address_1\"].apply(_std_street)\n",
    "building_info[\"address_line2_clean\"] = building_info[\"address_2\"].apply(_std_unit)\n",
    "\n",
    "building_info[\"borough_clean\"] = [\n",
    "    _borough_from_fields(b, c, ci)\n",
    "    for b, c, ci in zip(building_info.get(\"borough\"), building_info.get(\"county\"), building_info.get(\"city\"))\n",
    "]\n",
    "building_info[\"city_clean\"] = [\n",
    "    _usps_city_from_borough(ci, bn)\n",
    "    for ci, bn in zip(building_info.get(\"city\"), building_info[\"borough_clean\"])\n",
    "]\n",
    "building_info[\"state_clean\"] = STATE_ABBR\n",
    "building_info[\"postal_code_5_clean\"] = building_info.get(\"postal_code\").apply(_std_zip5) if \"postal_code\" in building_info.columns else None\n",
    "\n",
    "# Mark rows missing coords\n",
    "building_info[\"needs_geocoding\"] = building_info[\"latitude\"].isna() | building_info[\"longitude\"].isna()\n",
    "\n",
    "# a stable address key to dedupe geocoding calls\n",
    "def addr_key(r):\n",
    "    parts = [\n",
    "        _norm_ws(r[\"address_line1_clean\"]) or \"\",\n",
    "        _norm_ws(r[\"address_line2_clean\"]) or \"\",\n",
    "        _norm_ws(r[\"city_clean\"]) or \"\",\n",
    "        STATE_ABBR,\n",
    "        _norm_ws(r[\"postal_code_5_clean\"]) or \"\",\n",
    "    ]\n",
    "    return \"|\".join(parts).upper()\n",
    "\n",
    "building_info[\"geocode_key\"] = building_info.apply(addr_key, axis=1)\n",
    "\n",
    "building_info.head(3)[[\n",
    "    \"property_id\",'bbl',\"address_1\",\"address_2\",\"city\",\"borough\",\"county\",\n",
    "    \"address_line1_clean\",\"address_line2_clean\",\"city_clean\",\"state_clean\",\"postal_code_5_clean\",\n",
    "    \"needs_geocoding\",\"geocode_key\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9cbedb-474e-4706-878f-9706523bae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(building_info.shape)\n",
    "building_info = building_info.dropna(subset=[\"address_1\",\"address_2\",\"city\",\"postcode\",\"county\",\"borough\",\n",
    "                                             \"latitude\",\"longitude\",\"address_line1_clean\"],how='all')\n",
    "print(building_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f35295-c784-4028-a8d8-308023d43d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Geocode The Null lat long \n",
    "with_coord = building_info[building_info[\"needs_geocoding\"]==False]\n",
    "print(with_coord.shape)\n",
    "without_coord = building_info[building_info[\"needs_geocoding\"]==True]\n",
    "print(without_coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950c3dc-5212-409e-9419-0b877749bc0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "without_coord['city'][without_coord['city']=='Flushinig']='Flushing'\n",
    "without_coord['borough_clean'][(without_coord['city'].isin(['Sunnyside', 'Astoria', 'Jackson Heights', 'Flushing', 'Rego Park',\n",
    "                                                   'Forest Hills', 'Oakland Gardens', 'Bayside', 'Kew Gardens','flushing',\n",
    "                                                   'Woodside', 'Flushinig', 'Larchmont', 'Ridgewood','Jamaica', 'Albertson'])\n",
    "                                &(without_coord['borough_clean'].isnull()))]='QUEENS'\n",
    "### Manual Fixes where possible \n",
    "without_coord[\"borough_clean\"][((without_coord['borough_clean'].isnull())&\n",
    "               (without_coord[\"address_1\"]==\"3115 brighton 6th\"))]=\"BROOKLYN\"\n",
    "without_coord[\"borough_clean\"][((without_coord['borough_clean'].isnull())&\n",
    "               (without_coord[\"address_1\"]==\"144-35/39 Sanford Avenue\"))]=\"QUEENS\"\n",
    "without_coord[\"borough_clean\"][((without_coord['borough_clean'].isnull())&\n",
    "               (without_coord[\"address_1\"]==\"183-11 Hillside Ave\"))]=\"QUEENS\"\n",
    "without_coord[\"borough_clean\"][((without_coord['borough_clean'].isnull())&\n",
    "               (without_coord[\"address_1\"]==\"103-30/26 68th Ave\"))]=\"QUEENS\"\n",
    "without_coord[\"borough_clean\"][((without_coord['borough_clean'].isnull())&\n",
    "               (without_coord[\"address_1\"]==\"71-11 -71-23 162ND STREET\"))]=\"QUEENS\"\n",
    "without_coord = without_coord[~without_coord['borough_clean'].isnull()]\n",
    "print(without_coord.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd1a2d-417d-41da-8f92-c8b5c7cd8962",
   "metadata": {},
   "source": [
    "### Start of Geocoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c72dc-42fa-4a7b-992b-f09ecffc047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GEOCODING LOOP (with retries/candidates/NYC bias) ----\n",
    "results, failed = [], []\n",
    "\n",
    "def geocode_row(row):\n",
    "    cands = build_candidates(row)\n",
    "    # Always add *raw borough + state* fallback for named complexes (e.g., NYCHA) to try to at least anchor\n",
    "    borough = _borough_from_fields(row.get(\"borough_clean\"), row.get(\"county\"), row.get(\"city_clean\"))\n",
    "    if borough:\n",
    "        cands.append((f\"{borough.title()}, NY\", {\"administrative_area\": \"NY\", \"locality\": borough.title()}, \"BOROUGH_ONLY\"))\n",
    "\n",
    "    for query, comps, tag in cands:\n",
    "        if not query:\n",
    "            continue\n",
    "        try:\n",
    "            resp = gmaps.geocode(\n",
    "                query,\n",
    "                region=\"us\",\n",
    "                components=comps,\n",
    "                bounds=NYC_BOUNDS\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Geocode error:\", e)\n",
    "            resp = []\n",
    "\n",
    "        if resp:\n",
    "            # prefer rooftop or range_interpolated results\n",
    "            best = sorted(resp, key=lambda r: {\"ROOFTOP\":0, \"RANGE_INTERPOLATED\":1}.get(r.get(\"geometry\",{}).get(\"location_type\",\"\"), 2))[0]\n",
    "            loc = best[\"geometry\"][\"location\"]\n",
    "            return loc[\"lat\"], loc[\"lng\"], tag, best.get(\"formatted_address\"), best.get(\"place_id\")\n",
    "\n",
    "    return None, None, \"NO_HIT\", None, None\n",
    "\n",
    "# assumes without_coord is a DataFrame with the relevant columns\n",
    "for i, r in without_coord[[\"property_id\",'bbl',\"geocode_key\",\"address_line1_clean\",\"address_line2_clean\",\"borough_clean\",\"city_clean\",\"county\"]].drop_duplicates().iterrows():\n",
    "    # keep your i >= guard if needed\n",
    "    temp_df = pd.DataFrame([r])\n",
    "    lat, lng, tag, faddr, pid = geocode_row(r)\n",
    "\n",
    "    if lat is not None and lng is not None:\n",
    "        temp_df[\"latitude\"]  = lat\n",
    "        temp_df[\"longitude\"] = lng\n",
    "        temp_df[\"geocode_tag\"] = tag\n",
    "        temp_df[\"formatted_address\"] = faddr\n",
    "        temp_df[\"place_id\"] = pid\n",
    "        results.append(temp_df)\n",
    "    else:\n",
    "        temp_df[\"geocode_tag\"] = tag  # e.g., POBOX, MISSING_STREET, NO_HIT\n",
    "        failed.append(temp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a3cce-f6bd-4ac1-a33a-d9dda496f093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(results))\n",
    "print(len(failed)) ## Zero failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5898d21-ca1b-47b2-9fff-65b532c51541",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4569234-1ea4-4758-a810-69d74f2320f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Property ID Present Without_coord (Checking)\n",
    "print(without_coord.shape)\n",
    "without_coord_geo = without_coord.dropna(how='all',axis=1).merge(results_df, \n",
    "                                                                 on=[\"property_id\",'bbl',\"geocode_key\",\"address_line1_clean\",\"address_line2_clean\",\"borough_clean\",\"city_clean\",\"county\"],\n",
    "                                                                 how='left')\n",
    "\n",
    "without_coord_geo[without_coord_geo['latitude'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6051b7-10ad-4202-a4bd-7a5ce03843cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_info_geocoded= pd.concat([with_coord,without_coord_geo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0f36f-ef28-4077-8cc1-ecd348a0a03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Limit the resutlsd based on Building-specific Lat Long, not borough centroids.\n",
    "building_info_geocoded = building_info_geocoded[building_info_geocoded['geocode_tag'].isnull()]\n",
    "### NEEED TO ADD THE BBL BY ADDRESS.(Where needed)\n",
    "print(building_info_geocoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d7bf97-39f1-4683-8ac7-43313dbed8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "need_bbl = building_info_geocoded[((building_info_geocoded['bbl'].isnull())\n",
    "                                   |(building_info_geocoded['bbl'].astype(str).str.contains(\";\")))]\n",
    "print(need_bbl.shape)\n",
    "has_bbl = building_info_geocoded[((~building_info_geocoded['bbl'].isnull())\n",
    "                                  &(~building_info_geocoded['bbl'].astype(str).str.contains(\";\")))]\n",
    "print(has_bbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a60f6-f9ad-49ee-ade6-ed75c0584bed",
   "metadata": {},
   "source": [
    "### Enriching the Geocoded Building Information with BBLs using another API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c68887-b6ab-4e2c-8356-15a100424159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = need_bbl.copy()\n",
    "rows = []\n",
    "\n",
    "with requests.Session() as s:\n",
    "    for i, r in df.iterrows():\n",
    "        addr = str(r.get(\"address_line1_clean\", \"\")).strip()\n",
    "        boro = str(r.get(\"borough_clean\", \"\")).strip()\n",
    "        if not addr:\n",
    "            rows.append((i, None, None, None, None, None)); continue\n",
    "\n",
    "        q = f\"{addr}, {boro}, NY\" if boro else f\"{addr}, NY\"\n",
    "        try:\n",
    "            resp = s.get(\"https://geosearch.planninglabs.nyc/v2/search\",\n",
    "                         params={\"text\": q, \"size\": 5}, timeout=20)\n",
    "            feats = resp.json().get(\"features\", [])\n",
    "            # pick the best feature: prefer layer=='address' and same borough; else highest confidence\n",
    "            best, best_score = None, (-1, -1.0)\n",
    "            for f in feats:\n",
    "                p = f.get(\"properties\", {})\n",
    "                layer = p.get(\"layer\")\n",
    "                conf  = float(p.get(\"confidence\", 0) or 0)\n",
    "                fb    = (p.get(\"borough\") or ((p.get(\"addendum\") or {}).get(\"pad\") or {}).get(\"boroughName\") or \"\")\n",
    "                score = (2 if layer == \"address\" else 0) + (1 if boro and boro.upper() in str(fb).upper() else 0)\n",
    "                if (score, conf) > best_score:\n",
    "                    best, best_score = f, (score, conf)\n",
    "\n",
    "            if best:\n",
    "                props = best.get(\"properties\", {})\n",
    "                add   = (props.get(\"addendum\") or {}).get(\"pad\") or {}\n",
    "                bbl   = add.get(\"bbl\") or props.get(\"bbl\")\n",
    "                bin_  = add.get(\"bin\") or props.get(\"bin\")\n",
    "\n",
    "                # normalize BBL to a 10-digit string\n",
    "                if isinstance(bbl, str):\n",
    "                    digits = re.sub(r\"\\D\", \"\", bbl)\n",
    "                    bbl = digits.zfill(10) if digits else None\n",
    "                elif bbl is not None:\n",
    "                    try: bbl = str(int(bbl)).zfill(10)\n",
    "                    except: bbl = None\n",
    "\n",
    "                rows.append((i, bbl, bin_, props.get(\"confidence\"), props.get(\"label\") or props.get(\"name\"), q))\n",
    "            else:\n",
    "                rows.append((i, None, None, None, None, q))\n",
    "\n",
    "        except Exception as e:\n",
    "            rows.append((i, None, None, None, f\"ERR:{type(e).__name__}\", q))\n",
    "\n",
    "        time.sleep(0.1)  # tiny throttle to be polite\n",
    "\n",
    "# assemble + join back\n",
    "out = pd.DataFrame(rows, columns=[\"__idx\",\"BBL_from_api\",\"BIN_from_api\",\"confidence\",\"match_label\",\"query\"]).set_index(\"__idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de5529-283c-45e4-a99c-d408cb6d2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Putting all tegehter and cleaning \n",
    "need_bbl_enr = need_bbl.join(out)\n",
    "building_info_geocoded= pd.concat([need_bbl_enr, has_bbl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393b5b5-4c80-434a-84e9-932ef803f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning up some\n",
    "building_info_geocoded['bbl'] = building_info_geocoded['BBL_from_api'].combine_first(building_info_geocoded['bbl'])\n",
    "building_info_geocoded = building_info_geocoded.drop(columns=[\"BBL_from_api\",\"BIN_from_api\",\"confidence\"])\n",
    "building_info_geocoded = building_info_geocoded.dropna(how='all',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd7343-2ee2-4bfc-8ec9-75dcd2554189",
   "metadata": {},
   "source": [
    "### Pulling in Pluto Data for Num Floors and Height etc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea81e7-af29-41fc-8190-333896d0fbd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SOURCE # https://www.nyc.gov/content/planning/pages/resources/datasets/mappluto-pluto-change#mappluto\n",
    "# ===================== CONFIG =====================\n",
    "SRC   = r\"C:/Users/johnf/Downloads/nyc_mappluto_25v3_fgdb/MapPLUTO25v3.gdb\"\n",
    "LAYER = \"MapPLUTO_25v3_clipped\"\n",
    "\n",
    "# PLUTO columns you need (no geometry necessary)\n",
    "PLUTO_COLS = [\"BBL\",\"LandUse\",\"NumFloors\",\"UnitsRes\",\"BldgClass\",\"Block\",\"Borough\",\"BoroCode\"]\n",
    "\n",
    "# How many distinct BLOCKS per chunk (per borough)\n",
    "BLOCKS_PER_CHUNK = 2000  # tune: 1000â€“4000 are typical sweet spots\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def norm_bbl_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize to 10-digit BBL strings; handle '1234567890.0' correctly.\"\"\"\n",
    "    # Convert to string & trim\n",
    "    s = s.astype(str).str.strip()\n",
    "\n",
    "    # If it looks like 'digits.0' or 'digits.00', drop the decimal part\n",
    "    s = s.str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "\n",
    "    # Remove any remaining non-digits, then pad to 10\n",
    "    s = s.str.replace(r\"\\D\", \"\", regex=True).str.zfill(10)\n",
    "\n",
    "    # Turn \"0000000000\" into NA\n",
    "    return s.mask(s.eq(\"0000000000\"))\n",
    "\n",
    "def boro_code_from_bbl_str(bbl: str) -> int:\n",
    "    \"\"\"First digit of BBL is borough code 1..5.\"\"\"\n",
    "    return int(bbl[0]) if isinstance(bbl, str) and bbl and bbl[0].isdigit() else None\n",
    "\n",
    "def block_from_bbl_str(bbl: str) -> int:\n",
    "    \"\"\"Digits 2..6 (5 digits) are block.\"\"\"\n",
    "    try:\n",
    "        return int(bbl[1:6])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def pull_where(where: str, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Run filtered attribute read; returns pandas.DataFrame (no geometry).\"\"\"\n",
    "    return pyogrio.read_dataframe(\n",
    "        SRC, layer=LAYER,\n",
    "        columns=[c for c in cols if c != \"geometry\"],\n",
    "        read_geometry=False,\n",
    "        where=where,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb834c1-6874-42ff-a1f7-5bc35d74b447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===================== 1) LL84 BBL PREP =====================\n",
    "ll84 = building_info_geocoded.copy()\n",
    "ll84[\"bbl\"] = norm_bbl_series(ll84[\"bbl\"])\n",
    "ll84 = ll84[ll84[\"bbl\"].notna()].copy()\n",
    "\n",
    "bbls = ll84[\"bbl\"].unique().tolist()\n",
    "print(f\"[prep] Unique LL84 BBLs: {len(bbls):,}\")\n",
    "\n",
    "# ===================== 2) PULL PLUTO BY BBL IN(...) BATCHES =====================\n",
    "parts = []\n",
    "t0 = time.time()\n",
    "\n",
    "BATCH = 800   # tune if needed\n",
    "total_chunks = math.ceil(len(bbls) / BATCH)\n",
    "print(f\"[plan] {total_chunks} PLUTO chunks via BBL IN(...) batches\")\n",
    "\n",
    "for ci in range(total_chunks):\n",
    "    chunk = bbls[ci*BATCH:(ci+1)*BATCH]\n",
    "\n",
    "    # We already know \"BBL = '...'\" works, so quote as text\n",
    "    in_list = \",\".join(f\"'{bb}'\" for bb in chunk)\n",
    "    where = f\"BBL IN ({in_list})\"\n",
    "\n",
    "    part = pull_where(where, PLUTO_COLS)\n",
    "    print(\n",
    "        f\"[pull] chunk {ci+1}/{total_chunks} | requested {len(chunk):,} BBLs \"\n",
    "        f\"| pulled {len(part):,}\"\n",
    "    )\n",
    "\n",
    "    if not part.empty:\n",
    "        part[\"BBL\"] = norm_bbl_series(part[\"BBL\"])\n",
    "        parts.append(part)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "pluto_by_bbl = (\n",
    "    pd.concat(parts, ignore_index=True) if parts\n",
    "    else pd.DataFrame(columns=PLUTO_COLS)\n",
    ")\n",
    "\n",
    "print(f\"[done] Kept PLUTO rows: {len(pluto_by_bbl):,} in {elapsed:,.1f}s\")\n",
    "\n",
    "# ===================== 3) MERGE BACK TO LL84 + FILTER UNDER 5 STORIES =====================\n",
    "ll84_enriched = ll84.merge(\n",
    "    pluto_by_bbl.drop_duplicates(\"BBL\"),\n",
    "    left_on=\"bbl\",\n",
    "    right_on=\"BBL\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "ll84_enriched[\"stories\"] = ll84_enriched[\"NumFloors\"]\n",
    "\n",
    "res_codes = {\"01\", \"02\", \"03\", \"04\"}\n",
    "is_res = (\n",
    "    ll84_enriched[\"LandUse\"].astype(str).isin(res_codes)\n",
    "    | (ll84_enriched[\"UnitsRes\"].fillna(0) > 0)\n",
    "    | ll84_enriched.get(\n",
    "        \"BldgClass\",\n",
    "        pd.Series(index=ll84_enriched.index, dtype=\"object\")\n",
    "      ).fillna(\"\").str.startswith((\"A\", \"B\"))\n",
    ")\n",
    "\n",
    "under5 = ll84_enriched[\n",
    "    is_res\n",
    "    & ll84_enriched[\"stories\"].notna()\n",
    "    & (ll84_enriched[\"stories\"] < 5)\n",
    "].copy()\n",
    "\n",
    "print(\n",
    "    f\"[summary] LL84 rows: {len(ll84_enriched):,} | \"\n",
    "    f\"Residential <5 stories: {len(under5):,}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25840340-1850-4737-8ee4-f274325936f7",
   "metadata": {},
   "source": [
    "### Spatial joining with CT to pull ct into the buildings data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e9689-5288-4fe0-8544-ceb8710a7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "CT2010_URL = \"https://data.cityofnewyork.us/resource/bmjq-373p.geojson?$limit=50000\"\n",
    "resp = requests.get(CT2010_URL, timeout=60)\n",
    "resp.raise_for_status()\n",
    "\n",
    "ct2010 = gpd.read_file(io.BytesIO(resp.content)).to_crs(2263)\n",
    "ct2010.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca480d0-556a-4814-848d-8a2d6508608d",
   "metadata": {},
   "source": [
    "### Adding CT to buildigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6494e-9232-4a07-853a-aa2187420a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Points from your geocoded table ---\n",
    "# assumes working_residential_geo has columns: property_id, latitude, longitude\n",
    "pts = gpd.GeoDataFrame(\n",
    "    ll84_enriched[[\"property_id\", \"latitude\", \"longitude\"]].copy(),\n",
    "    geometry=gpd.points_from_xy(\n",
    "        ll84_enriched[\"longitude\"], ll84_enriched[\"latitude\"]\n",
    "    ),\n",
    "    crs=4326  # your geocodes are WGS84\n",
    ")\n",
    "\n",
    "# project points to match the CT layer (your CT layer is already to_crs(2263))\n",
    "pts_2263 = pts.to_crs(2263)\n",
    "\n",
    "# --- 2) Keep only what you need from the CT layer ---\n",
    "# 'ct2010' and/or 'boroct2010' are the usual tract IDs in that NYC layer\n",
    "ct_keep = ct2010[[\"ct2010\", \"boroct2010\", \"boroname\", \"geometry\"]].copy()\n",
    "\n",
    "# --- 3) Spatial join: which tract polygon contains each point ---\n",
    "joined = gpd.sjoin(\n",
    "    pts_2263,\n",
    "    ct_keep,\n",
    "    how=\"left\",\n",
    "    predicate=\"within\"   # points that fall inside a tract polygon\n",
    ")\n",
    "\n",
    "# If a property_id appears multiple times (duplicates), keep the first tract hit\n",
    "joined = joined.sort_index().drop_duplicates(subset=[\"property_id\"])\n",
    "\n",
    "# --- 4) Bring tract columns back to your original dataframe ---\n",
    "cols_to_add = [\"ct2010\", \"boroct2010\", \"boroname\"]\n",
    "ll84_enriched_ct = ll84_enriched.merge(\n",
    "    joined[[\"property_id\"] + cols_to_add],\n",
    "    on=\"property_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a5dee6-b618-4bc6-a669-8e8f2278a7f8",
   "metadata": {},
   "source": [
    "#### Pulling in the Building Footprionts of the buildings in the LL84 Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a30c2-7f1c-4e09-b174-9e1dbb0b9019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit = 50000\n",
    "offset = 0\n",
    "chunks = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://data.cityofnewyork.us/resource/5zhs-2jue.geojson?$limit={limit}&$offset={offset}&$order=:id\"\n",
    "    r = requests.get(url, timeout=120); r.raise_for_status()\n",
    "    g = gpd.read_file(io.BytesIO(r.content))\n",
    "    if g.empty:\n",
    "        break\n",
    "    if g.crs is None:\n",
    "        g = g.set_crs(4326)\n",
    "    chunks.append(g)\n",
    "    offset += limit\n",
    "    # optional progress\n",
    "    print(f\"Fetched {len(g)} rows (total {sum(len(c) for c in chunks)})\")\n",
    "    if len(g) < limit:\n",
    "        break\n",
    "    time.sleep(0.2)  \n",
    "\n",
    "building_ft_prnt = gpd.GeoDataFrame(pd.concat(chunks, ignore_index=True), crs=4326)\n",
    "\n",
    "# De-dup by Socrata row id if present\n",
    "if ':id' in building_ft_prnt.columns:\n",
    "    building_ft_prnt = building_ft_prnt.drop_duplicates(subset=':id')\n",
    "\n",
    "# Project to StatePlane feet for spatial ops\n",
    "building_ft_prnt_2263 = building_ft_prnt.to_crs(2263)\n",
    "\n",
    "print(building_ft_prnt_2263.shape, building_ft_prnt_2263.crs)\n",
    "building_ft_prnt_2263.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cd64c-0f7f-4486-839c-dea11e9aa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Very laerge df so limiting to those BBLs that exist in my data for latlong / LL84\n",
    "ll84_enriched_ct['bbl'] = ll84_enriched_ct['bbl'].astype(int).astype(str)\n",
    "print(building_ft_prnt_2263.shape)\n",
    "building_footprint_lim = building_ft_prnt_2263[building_ft_prnt_2263[\"base_bbl\"].isin(ll84_enriched_ct[\"bbl\"].unique())]\n",
    "print(building_footprint_lim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7910f-0137-430d-9d07-fb5936c4ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbl_counts_ftpnt = building_footprint_lim.groupby(['base_bbl']).agg({\"last_edited_date\":'nunique'}).reset_index()\n",
    "bbl_counts_ftpnt.reset_index(drop=True,inplace=True)\n",
    "\n",
    "single = bbl_counts_ftpnt[bbl_counts_ftpnt['last_edited_date']==1]\n",
    "print(single.shape)\n",
    "mult = bbl_counts_ftpnt[bbl_counts_ftpnt['last_edited_date']>1]\n",
    "print(mult.shape)\n",
    "\n",
    "### Limtiing the df \n",
    "building_ft_prnt_2263_mult = building_footprint_lim[building_footprint_lim['base_bbl'].isin(mult['base_bbl'].unique())]\n",
    "building_ft_prnt_2263_single= building_footprint_lim[building_footprint_lim['base_bbl'].isin(single['base_bbl'].unique())]\n",
    "\n",
    "## Extracing the year for mult\n",
    "building_ft_prnt_2263_mult['year'] = building_ft_prnt_2263_mult[\"last_edited_date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0f764-7bba-40ef-a6fb-2418162bc071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mult_year = []\n",
    "single_year = []\n",
    "for bbl in building_ft_prnt_2263_mult['base_bbl'].unique():\n",
    "    temp_df = building_ft_prnt_2263_mult[building_ft_prnt_2263_mult['base_bbl']==bbl]\n",
    "    min_year = temp_df['year'].min()\n",
    "    print(min_year)\n",
    "    max_year = temp_df['year'].max()\n",
    "    print(max_year)\n",
    "    if min_year==max_year:\n",
    "        single_year.append(temp_df[temp_df[\"last_edited_date\"]==temp_df[\"last_edited_date\"].max()])\n",
    "    else: \n",
    "        if min_year>=2017:\n",
    "            single_year.append(temp_df[temp_df[\"last_edited_date\"]==temp_df[\"last_edited_date\"].min()])\n",
    "        else: \n",
    "            if max_year ==2017:\n",
    "                sub_df = temp_df[temp_df['year']!=2017]\n",
    "                if sub_df['year'].max()<=2010:\n",
    "                    selection = sub_df[sub_df['last_edited_date'] == sub_df['last_edited_date'].max()]\n",
    "                    if selection.shape[0]>1:\n",
    "                        ## Taking the largest polygon \n",
    "                        single_year.append(selection[selection['shape_area'] == selection['shape_area'].max()])\n",
    "                    else:\n",
    "                        single_year.append(selection)\n",
    "                else:\n",
    "                    print(\"max year 2017, and there are other interim years. \")\n",
    "                    for yy in range(2010,2018):\n",
    "                        print(yy)\n",
    "                        year_lim_temp= temp_df[temp_df['year']<=yy]\n",
    "                        year_lim_temp = year_lim_temp[year_lim_temp['last_edited_date']==year_lim_temp['last_edited_date'].max()]\n",
    "                        print(year_lim_temp.shape)\n",
    "                        if year_lim_temp.shape[0]>1:\n",
    "                            ## Again takign the largest\n",
    "                            year_lim_temp = year_lim_temp[year_lim_temp['shape_area'] == year_lim_temp['shape_area'].max()]\n",
    "                        year_lim_temp['year_manual']=yy\n",
    "                        mult_year.append(year_lim_temp)\n",
    "            else:\n",
    "                print(\"max_year>2017\")\n",
    "                for yy in range(2010,2018):\n",
    "                    print(yy)\n",
    "                    year_lim_temp= temp_df[temp_df['year']<=yy]\n",
    "                    year_lim_temp = year_lim_temp[year_lim_temp['last_edited_date']==year_lim_temp['last_edited_date'].max()]\n",
    "                    print(year_lim_temp.shape)\n",
    "                    if year_lim_temp.shape[0]>1:\n",
    "                        ## Again takign the largest\n",
    "                        year_lim_temp = year_lim_temp[year_lim_temp['shape_area'] == year_lim_temp['shape_area'].max()]\n",
    "                    year_lim_temp['year_manual']=yy\n",
    "                    mult_year.append(year_lim_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d37c53-99af-4d89-8f22-517316c7fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_year_df = pd.concat(single_year)\n",
    "single_bbls_footprint = pd.concat([building_ft_prnt_2263_single,single_year_df.drop(columns=['year'])])\n",
    "mult_year_df = pd.concat(mult_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8afdd-20ce-4ba2-9d58-633d3f681ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Two step join via BBL \n",
    "ll84_footprint_1 = ll84_enriched_ct.merge(single_bbls_footprint, how='left', left_on=['bbl'], right_on=['base_bbl'])\n",
    "success1 = ll84_footprint_1[~ll84_footprint_1['base_bbl'].isnull()]\n",
    "print(success1.shape)\n",
    "not_success = ll84_footprint_1[ll84_footprint_1['base_bbl'].isnull()].dropna(how='all',axis=1)\n",
    "print(not_success.shape)\n",
    "#### success1 df is finalized building-specific information with the building footprint joined \n",
    "#### not_sccess df is the muiltiple year bbls, with changes in footprint for specific years, needs to be joined to year-specific energy building data before lidar spatial join. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9dd02-94a9-4893-8b08-5c9b8cea82ef",
   "metadata": {},
   "source": [
    "### Have Building Information joined with and the Canopy information, need to join back with the energy usage data (LL84)for year over year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6c473-9bc1-4424-8775-34df692a81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Limiting Energy Data tp those that have \"Whole Building\" metered eneryg area, but extrapolating for earlier years. \n",
    "residential3_2010_2017_lim1 = residential3_2010_2017[(\n",
    "    ((residential3_2010_2017[\"metered_areas_energy\"]==\"Whole Building\")&(residential3_2010_2017[\"year_ending_year\"]>=2012))\n",
    "     |(((residential3_2010_2017[\"metered_areas_energy\"]==\"Whole Building\")|(residential3_2010_2017[\"metered_areas_energy\"].isnull()))\n",
    "       &(residential3_2010_2017[\"year_ending_year\"]<=2012))\n",
    "    )]\n",
    "### Ensuring no relevant alerts for Energy Related meters \n",
    "residential3_2010_2017_lim2 = residential3_2010_2017_lim1[(\n",
    "    ((residential3_2010_2017_lim1['alert_energy_no_meters'].isnull())|(residential3_2010_2017_lim1['alert_energy_no_meters']=='Ok'))\n",
    "    &((residential3_2010_2017_lim1['alert_energy_meter_has_less'].isnull())|(residential3_2010_2017_lim1['alert_energy_meter_has_less']=='Ok'))\n",
    "    &((residential3_2010_2017_lim1['alert_energy_meter_has_gaps'].isnull())|(residential3_2010_2017_lim1['alert_energy_meter_has_gaps']=='Ok'))\n",
    "    &((residential3_2010_2017_lim1['alert_energy_meter_has'].isnull())|(residential3_2010_2017_lim1['alert_energy_meter_has']=='Ok'))\n",
    "    &((residential3_2010_2017_lim1['alert_energy_meter_has_single'].isnull())|(residential3_2010_2017_lim1['alert_energy_meter_has_single']=='Ok'))\n",
    "    &((residential3_2010_2017_lim1['alert_meter_has_less_than'].isnull())|(residential3_2010_2017_lim1['alert_meter_has_less_than']=='Ok'))\n",
    "    &((residential3_2010_2017_lim1['alert_no_meters_are_associated'].isnull())|(residential3_2010_2017_lim1['alert_no_meters_are_associated']=='Ok'))\n",
    "    &((residential3_2010_2017_lim1['alert_meter_has_overlaps'].isnull())|(residential3_2010_2017_lim1['alert_meter_has_overlaps']=='Ok'))\n",
    ")]\n",
    "### Dropping filtered columns \n",
    "residential3_2010_2017_lim2 = residential3_2010_2017_lim2.drop(columns=[\"metered_areas_energy\",\"alert_energy_no_meters\",\"alert_energy_meter_has_less\",\n",
    "                                                                        \"alert_energy_meter_has_gaps\",\"alert_energy_meter_has\",\"alert_energy_meter_has_single\",\n",
    "                                                                        \"alert_meter_has_less_than\",\"alert_no_meters_are_associated\",\"alert_meter_has_overlaps\"])\n",
    "\n",
    "### limiting to where number_of_buildings is 1 to keep simple, some years onlu have null values. \n",
    "residential3_2010_2017_lim3 = residential3_2010_2017_lim2[(\n",
    "    ((residential3_2010_2017_lim2[\"number_of_buildings\"]==1)&(residential3_2010_2017_lim2['year_ending_year']>=2013))\n",
    "    |(((residential3_2010_2017_lim2[\"number_of_buildings\"]==1)|(residential3_2010_2017_lim2[\"number_of_buildings\"].isnull()))\n",
    "      &(residential3_2010_2017_lim2['year_ending_year']<2013))\n",
    ")]\n",
    "### Generalized Way to limit to mutifamily residential buildings\n",
    "multifamily_residential_cols = [i for i in residential3_2010_2017_lim3.columns if 'multifamily_' in i]\n",
    "residential3_2010_2017_lim4 = residential3_2010_2017_lim3.dropna(subset=multifamily_residential_cols, how=\"all\")\n",
    "\n",
    "### Further Facility Type / Property Use Limiting\n",
    "residential3_2010_2017_lim4 = residential3_2010_2017_lim4[(\n",
    "    ((residential3_2010_2017_lim4[\"year_ending_year\"]<2012) & (residential3_2010_2017_lim4[\"facility_type\"]=='Multifamily Housing'))\n",
    "    |((residential3_2010_2017_lim4[\"year_ending_year\"]>=2012)&(residential3_2010_2017_lim4[\"facility_type\"].isnull()))\n",
    ")]\n",
    "\n",
    "### Only Non estimated data, but also many nulls in this field\n",
    "residential3_2010_2017_lim4 = residential3_2010_2017_lim4[(\n",
    "    ((residential3_2010_2017_lim4['estimated_data_flag'].astype(str).str.upper().str.strip()=='NO')\n",
    "     |(residential3_2010_2017_lim4['estimated_data_flag'].isnull()))\n",
    "    &((residential3_2010_2017_lim4[\"temporary_data_flag\"].isnull())\n",
    "      |(residential3_2010_2017_lim4[\"temporary_data_flag\"].astype(str).str.upper().str.strip()=='NO'))\n",
    "    &((residential3_2010_2017_lim4[\"estimated_data_flag_1\"].isnull())\n",
    "      |(residential3_2010_2017_lim4[\"estimated_data_flag_1\"].astype(str).str.upper().str.strip()=='NO'))\n",
    "    &((residential3_2010_2017_lim4[\"estimated_values_energy\"].isnull())\n",
    "      |(residential3_2010_2017_lim4[\"estimated_values_energy\"].astype(str).str.upper().str.strip()=='NO'))\n",
    "    &((residential3_2010_2017_lim4[\"alert_property_has_no_uses\"].isnull())\n",
    "      |(residential3_2010_2017_lim4[\"alert_property_has_no_uses\"].astype(str).str.upper().str.strip()=='OK'))\n",
    "    &((residential3_2010_2017_lim4[\"default_values\"].isnull())\n",
    "      |(residential3_2010_2017_lim4[\"default_values\"].astype(str).str.upper().str.strip()=='NO'))\n",
    "    &((residential3_2010_2017_lim4[\"temporary_values\"].isnull())\n",
    "      |(residential3_2010_2017_lim4[\"temporary_values\"].astype(str).str.upper().str.strip()=='NO'))\n",
    "    \n",
    ")]\n",
    "\n",
    "## Dropping more columns already filtered on, but not needed for this analysis, also other columns not needed 9Less Noise to sift through for consolidation)\n",
    "water_cols = [i for i in residential3_2010_2017_lim4.columns if 'water' in i]\n",
    "national_metric_cols = [i for i in residential3_2010_2017_lim4.columns if 'national' in i]\n",
    "flag_cols = [i for i in residential3_2010_2017_lim4.columns if 'flag' in i]\n",
    "drinking_water = [i for i in residential3_2010_2017_lim4.columns if 'potable' in i]\n",
    "electricity = [i for i in residential3_2010_2017_lim4.columns if 'electricity' in i] # looking at total energy for buildings not elec. (varies percentage by building)\n",
    "other_specific_types = [i for i in residential3_2010_2017_lim4.columns if ('fuel' in i or 'steam' in i or 'diesel' in i or 'natural' in i )]\n",
    "financial = [i for i in residential3_2010_2017_lim4.columns if ('_cost' in i or 'cost_' in i or 'savings' in i )]\n",
    "greentech = [i for i in residential3_2010_2017_lim4.columns if 'green' in i]\n",
    "emissions = [i for i in residential3_2010_2017_lim4.columns if 'emission' in i]\n",
    "\n",
    "\n",
    "residential3_2010_2017_lim4 = residential3_2010_2017_lim4.drop(columns=list(set([\"number_of_buildings\",'estimated_values_energy']\n",
    "                                                                                +multifamily_residential_cols\n",
    "                                                                                +water_cols\n",
    "                                                                                +national_metric_cols\n",
    "                                                                                +flag_cols\n",
    "                                                                                + drinking_water\n",
    "                                                                                + electricity\n",
    "                                                                                +other_specific_types\n",
    "                                                                                +financial\n",
    "                                                                                +greentech\n",
    "                                                                                +emissions\n",
    "                                                                               )))\n",
    "## Droping all that are just null values for all entries\n",
    "residential3_2010_2017_lim4 = residential3_2010_2017_lim4.dropna(how='all',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9db02-61a5-4eeb-a6f1-d63aabb6fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking a look \n",
    "print(residential3_2010_2017_lim4.shape)\n",
    "residential3_2010_2017_lim4.groupby([\"year_ending_year\"]).agg({\"property_id\":\"nunique\",'bbl':\"nunique\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce74a02-9d56-4889-bd3f-540b8a218fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Limiting the residential3_2010_2017_lim4 df to those instances that buildinga are present for ALL 7 years\n",
    "property_id_2010 = residential3_2010_2017_lim4[\"property_id\"][((residential3_2010_2017_lim4['year_ending_year']==2010)\n",
    "                                                               &(~residential3_2010_2017_lim4[\"property_id\"].isnull()))].unique()\n",
    "bbl_2010 = residential3_2010_2017_lim4[\"bbl\"][((residential3_2010_2017_lim4['year_ending_year']==2010)\n",
    "                                                               &(~residential3_2010_2017_lim4[\"bbl\"].isnull()))].unique()\n",
    "consistant_presence = []\n",
    "for year in residential3_2010_2017_lim4[\"year_ending_year\"].unique():\n",
    "    print(year)\n",
    "    temp_lim_year = residential3_2010_2017_lim4[residential3_2010_2017_lim4[\"year_ending_year\"]==year].dropna(how='all',axis=1)\n",
    "    print(temp_lim_year.shape)\n",
    "    if year ==2010:\n",
    "        consistant_presence.append(temp_lim_year)\n",
    "        continue\n",
    "    else:\n",
    "        temp_lim = temp_lim_year[((temp_lim_year[\"bbl\"].isin(bbl_2010))|(temp_lim_year[\"property_id\"].isin(property_id_2010)))]\n",
    "        print(temp_lim.shape)\n",
    "        print(f\" Dropped {temp_lim_year.shape[0]-temp_lim.shape[0]} rows\")\n",
    "        consistant_presence.append(temp_lim)\n",
    "all_building_energy_2010_2017 = pd.concat(consistant_presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bc7fd-7d6f-458c-ab67-af6d16edb9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Finalizing the Column names \n",
    "all_building_energy_2010_2017[all_building_energy_2010_2017['year_ending_year']==2010].dropna(how='all',axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d97d1-d14f-468b-9f64-03ada43a874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_2010 = {\n",
    "    # Building area â†’ canonical GFA\n",
    "    \"total_floor_space_sq_ft\": \"gfa_building_ft2\",\n",
    "\n",
    "    # Energy: EUI & totals\n",
    "    \"current_site_energy_intensity\": \"site_eui_kbtu_ft\",\n",
    "    \"current_total_site_energy\": \"site_energy_use_kbtu\",\n",
    "    \"target_site_energy_intensity\": \"target_site_eui_kbtu_ft\",\n",
    "    \"current_total_source_energy\": \"source_energy_use_kbtu\",\n",
    "\n",
    "    # Normalized site energy index (not kBtu, but useful as separate indicator)\n",
    "    \"current_weather_normalized\": \"weather_normalized_site_energy_index\",\n",
    "\n",
    "    # Water & on-site energy\n",
    "    \"total_indoor_and_outdoor\": \"total_water_use_kgal\",\n",
    "    \"current_total_on_site\": \"on_site_energy_use_kbtu\",\n",
    "    # keep current_source_energy as-is or drop it; you already have source_energy_use_kbtu above\n",
    "}\n",
    "rename_2011 = {\n",
    "    # Building area\n",
    "    \"total_floor_space_sq_ft\": \"gfa_building_ft2\",\n",
    "\n",
    "    # Last modified â†’ canonical\n",
    "    \"last_modified_date\": \"last_modified_date_property\",\n",
    "\n",
    "    # Energy: EUI & totals\n",
    "    \"current_site_energy_intensity\": \"site_eui_kbtu_ft\",\n",
    "    \"current_total_site_energy\": \"site_energy_use_kbtu\",\n",
    "    \"target_site_energy_intensity\": \"target_site_eui_kbtu_ft\",\n",
    "    \"current_total_source_energy\": \"source_energy_use_kbtu\",\n",
    "\n",
    "    # Normalized site energy index\n",
    "    \"current_weather_normalized\": \"weather_normalized_site_energy_index\",\n",
    "\n",
    "    # Optional: you can keep current_source_energy as-is or ignore it\n",
    "}\n",
    "rename_2012 = {\n",
    "    # State field\n",
    "    \"state_province\": \"state\",\n",
    "\n",
    "    # Building area\n",
    "    \"property_floor_area_buildings\": \"gfa_building_ft2\",\n",
    "\n",
    "    # Last modified â†’ canonical\n",
    "    \"date_property_last_modified\": \"last_modified_date_property\",\n",
    "\n",
    "    # Energy fields already use canonical names:\n",
    "    #   site_eui_kbtu_ft\n",
    "    #   weather_normalized_site_eui\n",
    "    #   site_energy_use_kbtu\n",
    "    #   source_eui_kbtu_ft\n",
    "    #   weather_normalized_source\n",
    "    #   source_energy_use_kbtu\n",
    "    #   target_site_eui_kbtu_ft\n",
    "}\n",
    "rename_2013 = {\n",
    "    # Building area\n",
    "    \"property_floor_area_building\": \"gfa_building_ft2\",\n",
    "\n",
    "    # Last modified\n",
    "    \"date_property_last_modified\": \"last_modified_date_property\",\n",
    "\n",
    "    # Everything else you listed (energy) is already in canonical form:\n",
    "    #   site_eui_kbtu_ft\n",
    "    #   weather_normalized_site_eui\n",
    "    #   site_energy_use_kbtu\n",
    "    #   weather_normalized_site_energy\n",
    "    #   source_eui_kbtu_ft\n",
    "    #   weather_normalized_source\n",
    "    #   source_energy_use_kbtu\n",
    "    # plus the various weather_normalized_site* convenience columns\n",
    "}\n",
    "rename_2014 = {\n",
    "    # Building area\n",
    "    \"property_floor_area_building\": \"gfa_building_ft2\",\n",
    "\n",
    "    # Energy fields already canonical names as in 2013\n",
    "}\n",
    "rename_2015 = {\n",
    "    # Building area: self-reported & EPA-calculated\n",
    "    \"property_gfa_self_reported\": \"gfa_building_ft2\",\n",
    "    \"property_gfa_epa_calculated\": \"gfa_epa_total_ft2\",       # bldg + parking\n",
    "    \"property_gfa_epa_calculated_1\": \"gfa_epa_buildings_ft2\", # buildings only\n",
    "\n",
    "    # Last modified\n",
    "    \"date_property_last_modified\": \"last_modified_date_property\",\n",
    "\n",
    "    # (Energy columns already have canonical names)\n",
    "}\n",
    "rename_2016 = {\n",
    "    # Building area: self-reported & calculated\n",
    "    \"property_gfa_self_reported\": \"gfa_building_ft2\",\n",
    "    \"property_gfa_calculated\": \"gfa_epa_total_ft2\",       # equivalent to EPA-calculated total\n",
    "    \"property_gfa_calculated_1\": \"gfa_epa_buildings_ft2\", # buildings only\n",
    "\n",
    "    # Last modified\n",
    "    \"date_property_last_modified\": \"last_modified_date_property\",\n",
    "\n",
    "    # Energy:\n",
    "    #   site_eui_kbtu_ft\n",
    "    #   weather_normalized_site_eui\n",
    "    #   site_energy_use_kbtu\n",
    "    #   weather_normalized_site_energy\n",
    "    #   source_eui_kbtu_ft\n",
    "    #   weather_normalized_source\n",
    "    #   source_energy_use_kbtu\n",
    "    #   weather_normalized_source_1 (duplicate variant)\n",
    "    #   source_energy_use_adjusted\n",
    "    #   source_eui_adjusted_to_current\n",
    "    # all already match canon â†’ no rename needed\n",
    "}\n",
    "rename_2017 = {\n",
    "    # Building area\n",
    "    \"property_gfa_self_reported\": \"gfa_building_ft2\",\n",
    "    \"property_gfa_calculated\": \"gfa_epa_total_ft2\",\n",
    "    \"property_gfa_calculated_1\": \"gfa_epa_buildings_ft2\",\n",
    "\n",
    "    # Last modified already comes as last_modified_date_property for 2017 in your list.\n",
    "    # No rename required.\n",
    "\n",
    "    # Energy fields already match 2016â€™s canonical naming.\n",
    "}\n",
    "rename_maps = {\n",
    "    2010: rename_2010,\n",
    "    2011: rename_2011,\n",
    "    2012: rename_2012,\n",
    "    2013: rename_2013,\n",
    "    2014: rename_2014,\n",
    "    2015: rename_2015,\n",
    "    2016: rename_2016,\n",
    "    2017: rename_2017,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9892c28-2f72-491c-80a6-cab6592e8a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop over years, rename columns, and collect\n",
    "aligned_frames = []\n",
    "\n",
    "for year in sorted(all_building_energy_2010_2017[\"year_ending_year\"].unique()):\n",
    "    print(\"Processing year:\", year)\n",
    "    temp_lim_year = all_building_energy_2010_2017[\n",
    "        all_building_energy_2010_2017[\"year_ending_year\"] == year\n",
    "    ].copy()\n",
    "\n",
    "    # apply the year-specific renames (if we have one)\n",
    "    rename_dict = rename_maps.get(year, {})\n",
    "    temp_lim_year = temp_lim_year.dropna(how='all',axis=1).rename(columns=rename_dict)\n",
    "    if temp_lim_year.columns.duplicated().any():\n",
    "        break\n",
    "    print(temp_lim_year.columns)\n",
    "    aligned_frames.append(temp_lim_year)\n",
    "\n",
    "# 3) stitch them back together\n",
    "all_building_energy_2010_2017_renamed = pd.concat(aligned_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702bf41-5345-4008-b6bc-98fdbd9e7295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_cols = [\n",
    "    # Core / IDs / location\n",
    "    \"year_ending_year\",\n",
    "    \"property_id\",\n",
    "    \"bbl\",\n",
    "    \"address_1\",\n",
    "    \"address_2\",\n",
    "    \"city\",\n",
    "    \"borough\",\n",
    "    \"postcode\",\n",
    "    \"state\",\n",
    "\n",
    "    # Data / timing\n",
    "    \"release_date\",\n",
    "    \"generation_date\",\n",
    "    \"current_energy_period_ending\",\n",
    "    \"energy_current_date\",\n",
    "    \"period_ending_date\",\n",
    "    \"last_modified_date_property\",\n",
    "\n",
    "    # Building\n",
    "    \"year_built\",\n",
    "    \"gfa_building_ft2\",\n",
    "    \"gfa_epa_total_ft2\",\n",
    "    \"gfa_epa_buildings_ft2\",\n",
    "    \"largest_property_use_type_1\",\n",
    "    \"costar_property_id\",\n",
    "\n",
    "    # Energy â€“ core\n",
    "    \"site_eui_kbtu_ft\",\n",
    "    \"weather_normalized_site_eui\",\n",
    "    \"site_energy_use_kbtu\",\n",
    "    \"weather_normalized_site_energy\",\n",
    "    \"source_eui_kbtu_ft\",\n",
    "    \"weather_normalized_source\",\n",
    "    \"source_energy_use_kbtu\",\n",
    "    \"target_site_eui_kbtu_ft\",\n",
    "\n",
    "    # Energy â€“ early-year extras\n",
    "    \"weather_normalized_site_energy_index\",\n",
    "    \"current_source_energy\",\n",
    "    \"on_site_energy_use_kbtu\",\n",
    "    \"total_water_use_kgal\",\n",
    "\n",
    "    # Extra normalized variants\n",
    "    \"weather_normalized_source_1\",\n",
    "    \"weather_normalized_site\",\n",
    "    \"weather_normalized_site_1\",\n",
    "    \"weather_normalized_site_2\",\n",
    "    \"weather_normalized_site_3\",\n",
    "\n",
    "    # Adjusted source metrics\n",
    "    \"source_energy_use_adjusted\",\n",
    "    \"source_eui_adjusted_to_current\",\n",
    "]\n",
    "\n",
    "all_building_energy_2010_2017_renamed = all_building_energy_2010_2017_renamed[final_cols]\n",
    "# all_building_energy_2010_2017_renamed.to_csv(\"all_building_energy_2010_2017_renamed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f8fb9-3c47-4fcd-9e56-8ab722115f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally Joining back in the buidlings information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86de10-b951-43d2-bba4-f52992a95d3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_building_energy_2010_2017_renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0267bc-5fcb-4546-8c07-9fe0552d58d0",
   "metadata": {},
   "source": [
    "### Joining the energy building data and the Building Footprint data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44a844-103c-4279-a13a-fef1be2430f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joing with the footprints already \n",
    "success1\n",
    "print(success1.shape)\n",
    "### Needs to add footprints, but for year-specific ones \n",
    "not_success\n",
    "print(not_success.shape)\n",
    "\n",
    "### This is the footprint data for those that aren int success/ single bb; resolution data: mult_year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34426bbc-d0d2-45f2-9632-0ed81e34b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multi Step join \n",
    "join_columns_energy = [i for i in all_building_energy_2010_2017_renamed.columns if i not in ['borough','postcode','state']]\n",
    "all_building_energy_2010_2017_renamed\n",
    "\n",
    "## PART 1 \n",
    "final_building_energy_data1 = all_building_energy_2010_2017_renamed[join_columns_energy].merge(\n",
    "    success1,\n",
    "        how=\"inner\",\n",
    "        on=[\"property_id\",\"bbl\",\"address_1\",\"address_2\",\"city\"])\n",
    "print(\"final_building_energy_data1\")\n",
    "print(final_building_energy_data1.shape)\n",
    "print(\"final_building_energy_data1 bbls\")\n",
    "print(len(final_building_energy_data1['bbl'].unique()))\n",
    "\n",
    "print(\"not_success\")\n",
    "print(not_success.shape)\n",
    "print(\"not_success bbls\")\n",
    "print(len(not_success['bbl'].unique()))\n",
    "\n",
    "## Part 2 \n",
    "final_building_energy_data2 = all_building_energy_2010_2017_renamed[join_columns_energy].merge(\n",
    "    not_success,\n",
    "        how=\"inner\",\n",
    "        on=[\"property_id\",\"bbl\",\"address_1\",\"address_2\",\"city\"])\n",
    "print(\"final_building_energy_data2\")\n",
    "print(final_building_energy_data2.shape)\n",
    "print(\"final_building_energy_data2 bbls\")\n",
    "print(len(final_building_energy_data2['bbl'].unique()))\n",
    "\n",
    "### Checking Mutli Stats \n",
    "print(\"mult_year_df\")\n",
    "print(mult_year_df.shape)\n",
    "print('mult_year_df bbls')\n",
    "print(len(mult_year_df['base_bbl'].unique()))\n",
    "\n",
    "# final_building_energy_data1\n",
    "final_building_energy_data3 = final_building_energy_data2.merge(\n",
    "    mult_year_df,\n",
    "    how='inner', \n",
    "    right_on = ['base_bbl','year_manual'], \n",
    "    left_on = ['bbl','year_ending_year'])\n",
    "print(final_building_energy_data3.shape)\n",
    "print(len(final_building_energy_data3['bbl'].unique()))\n",
    "\n",
    "\n",
    "# final_building_energy_data1\n",
    "final_building_energy_data4 = final_building_energy_data2.merge(\n",
    "    mult_year_df,\n",
    "    how='left', \n",
    "    right_on = ['base_bbl','year_manual'], \n",
    "    left_on = ['bbl','year_ending_year'])\n",
    "print(final_building_energy_data4.shape)\n",
    "print(len(final_building_energy_data4['bbl'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e40143-bf81-4167-91b7-595afccafff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### putting it all together with waht was successful for multi year matching and singluelar values \n",
    "final_building_data = pd.concat([final_building_energy_data1,final_building_energy_data3])\n",
    "final_building_data = final_building_data.drop(columns = ['year_manual','year'])\n",
    "\n",
    "### SEMI FINAL WORKING DATASET WITH BUIDLGINDS\n",
    "print(final_building_data.groupby(['year_ending_year']).agg({\"bbl\":\"nunique\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46815b-2a3e-439a-916c-699d202048a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THREE WORKING DATA SETS FOR THE ANALYSIS \n",
    "\n",
    "### FIRST IS 2010 and 2017 years specifically to have more buildings for canopy analysis\n",
    "master_2010_2017_bbl_list = [b for b in \n",
    "                             list(final_building_data['bbl'][final_building_data['year_ending_year'] ==2010].unique()) \n",
    "                             if b in list(final_building_data['bbl'][final_building_data['year_ending_year'] ==2017].unique())]\n",
    "print(len(master_2010_2017_bbl_list))\n",
    "building_final_2010_2017 = final_building_data[((final_building_data['year_ending_year'].isin([2010,2017]))\n",
    "                                                &(final_building_data['bbl'].isin(master_2010_2017_bbl_list)))]\n",
    "building_final_2010_2017.to_csv('building_final_2010_2017_20251116.csv',index=False)\n",
    "\n",
    "### Second is only the BBLs that are present in every eyar, with a max possible number of 134 because od 2012 being so low.\n",
    "years = range(2010, 2018)  # adjust as needed\n",
    "bbl_sets = [set(final_building_data.loc[final_building_data['year_ending_year'] == y, 'bbl'].unique()) for y in years]\n",
    "common_bbls = set.intersection(*bbl_sets)  # BBLs present in ALL years\n",
    "common_bbls = list(common_bbls)           # back to list if you want\n",
    "print(len(common_bbls)) ## 25 bbls \n",
    "building_final_all_yrs = final_building_data[final_building_data['bbl'].isin(common_bbls)]\n",
    "print(building_final_all_yrs.shape)\n",
    "building_final_all_yrs.to_csv('building_final_all_yrs_20251116.csv',index=False)\n",
    "\n",
    "\n",
    "### OPTIONAL1 (third would be all bbls and buildings with energy data (reguardless of fotoprint data)\n",
    "max_buildings_no_footprint = pd.concat([final_building_energy_data1,final_building_energy_data4])\n",
    "years = range(2010, 2018)  # adjust as needed\n",
    "bbl_sets = [set(max_buildings_no_footprint.loc[max_buildings_no_footprint['year_ending_year'] == y, 'bbl'].unique()) for y in years]\n",
    "common_bbls = set.intersection(*bbl_sets)  # BBLs present in ALL years\n",
    "common_bbls = list(common_bbls)           # back to list if you want\n",
    "print(len(common_bbls)) ## 33  bbls \n",
    "max_buildings_no_footprint_all_years = max_buildings_no_footprint[max_buildings_no_footprint['bbl'].isin(common_bbls)]\n",
    "max_buildings_no_footprint_all_years = max_buildings_no_footprint_all_years.drop(columns = ['year_manual','year'])\n",
    "max_buildings_no_footprint_all_years.to_csv('max_buildings_no_footprint_all_years_20251116.csv',index=False)\n",
    "\n",
    "\n",
    "max_buildings_no_footprint = pd.concat([final_building_energy_data1,final_building_energy_data4])\n",
    "years = [2010, 2017]  # adjust as needed\n",
    "bbl_sets = [set(max_buildings_no_footprint.loc[max_buildings_no_footprint['year_ending_year'] == y, 'bbl'].unique()) for y in years]\n",
    "common_bbls = set.intersection(*bbl_sets)  # BBLs present in ALL years\n",
    "common_bbls = list(common_bbls)           # back to list if you want\n",
    "print(len(common_bbls)) ## 1664 bbls \n",
    "max_buildings_no_footprint_2010_2017 = max_buildings_no_footprint[max_buildings_no_footprint['bbl'].isin(common_bbls)]\n",
    "max_buildings_no_footprint_2010_2017 = max_buildings_no_footprint_2010_2017.drop(columns = ['year_manual','year'])\n",
    "max_buildings_no_footprint_2010_2017.to_csv('max_buildings_no_footprint_2010_2017_20251116.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6526b-c64a-48b8-b9c5-6ef5e536e93b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebb5d692-2288-402d-ac83-8a757bbe1d60",
   "metadata": {},
   "source": [
    "## Now That the building Data is finalized, need to add the Lidar for Canopy Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab912d-4bd9-48f1-9f8c-05cc0a11f3cd",
   "metadata": {},
   "source": [
    "### Tree Canopy Joining via spatial join with 50 foot buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808e64b-0e9c-4abd-b69a-cde76db9818f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path to your geodatabase (.gdb)\n",
    "# https://github.com/CityOfNewYork/nyc-geo-metadata/blob/main/Metadata/Metadata_TreeCanopyChange.md\n",
    "gdb_path = r\"C:\\Users\\johnf\\Downloads\\Tree_Canopy_Change (1)\\Tree_Canopy_Change\\NYC_TreeCanopyChange_2010_2017.gdb\"\n",
    "\n",
    "# List all layers in the geodatabase\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "print(\"Layers available:\", layers)\n",
    "\n",
    "# Read a specific layer into a GeoDataFrame\n",
    "gdf = gpd.read_file(gdb_path, layer=layers[0])  # Replace [0] with your desired layer\n",
    "print(gdf.head())\n",
    "\n",
    "# Optional: save as shapefile or GeoJSON\n",
    "# gdf.to_file(\"output.shp\") \n",
    "# gdf.to_file(\"output.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "canopy_change = gdf.copy()\n",
    "# canopy_change\n",
    "print(canopy_change.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4fb0d7-9aa2-4e6b-8dc8-bcad2aac96eb",
   "metadata": {},
   "source": [
    "### Geodataframe conversion for building_final_2010_2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbcb0a-d28d-451b-9a8f-bc5cbf7c8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from your existing df, e.g. building_final_2010_2017\n",
    "bdf = building_final_2010_2017.copy()\n",
    "# geometry already contains MultiPolygon objects, so just wrap as GeoDataFrame\n",
    "buildings_gdf = gpd.GeoDataFrame(bdf, geometry=\"geometry\", crs=\"EPSG:2263\")  # CRS in feet\n",
    "# 50-ft buffer around each building\n",
    "buildings_buffered = buildings_gdf.copy()\n",
    "buildings_buffered[\"geometry\"] = buildings_buffered.geometry.buffer(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b720cc1-b2dd-46ac-aed2-cd48445a7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure canopy_gdf is in same CRS\n",
    "canopy_change = canopy_change.to_crs(buildings_buffered.crs)\n",
    "\n",
    "# 2. Compute actual intersection geometries\n",
    "#    This creates one row per buildingâ€“canopy overlap\n",
    "intersections = gpd.overlay(\n",
    "    buildings_buffered[['bbl', 'geometry']],  # keep bbl + geometry\n",
    "    canopy_change,                               # keep all canopy attrs\n",
    "    how='intersection'\n",
    ")\n",
    "\n",
    "# 3. Compute overlap area for each pair (units: ftÂ² since EPSG:2263)\n",
    "intersections['overlap_area'] = intersections.geometry.area\n",
    "\n",
    "# 4. For each building (bbl), keep the row with the largest overlap_area\n",
    "idx_max = intersections.groupby('bbl')['overlap_area'].idxmax()\n",
    "largest_canopy_per_bldg = intersections.loc[idx_max].copy()\n",
    "\n",
    "largest_canopy_per_bldg = largest_canopy_per_bldg.rename(\n",
    "    columns={\"geometry\": \"canopy_geom\",\"Class\":\"canopy_change_class\"})\n",
    "\n",
    "buildings_with_canopy = buildings_buffered.merge(\n",
    "    largest_canopy_per_bldg[[\"bbl\", \"canopy_geom\", \"canopy_change_class\", \"overlap_area\"]],\n",
    "    on=\"bbl\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ecd81-7074-4ec1-94e6-5cfb73aa5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a look\n",
    "buildings_with_canopy.groupby(['canopy_change_class','year_ending_year']).agg({'bbl':\"nunique\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72affbc-bada-4560-866e-7c100e8507a4",
   "metadata": {},
   "source": [
    "### Geodataframe conversion for building_final_all_yrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c18e3d-896d-4972-8a46-256f4189b226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting from your existing df, e.g. building_final_all_yrs\n",
    "bdf = building_final_all_yrs.copy()\n",
    "# geometry already contains MultiPolygon objects, so just wrap as GeoDataFrame\n",
    "buildings_gdf = gpd.GeoDataFrame(bdf, geometry=\"geometry\", crs=\"EPSG:2263\")  # CRS in feet\n",
    "# 50-ft buffer around each building\n",
    "buildings_buffered = buildings_gdf.copy()\n",
    "buildings_buffered[\"geometry\"] = buildings_buffered.geometry.buffer(50)\n",
    "\n",
    "# ensure canopy_gdf is in same CRS\n",
    "canopy_change = canopy_change.to_crs(buildings_buffered.crs)\n",
    "\n",
    "# 2. Compute actual intersection geometries\n",
    "#    This creates one row per buildingâ€“canopy overlap\n",
    "intersections = gpd.overlay(\n",
    "    buildings_buffered[['bbl', 'geometry']],  # keep bbl + geometry\n",
    "    canopy_change,                               # keep all canopy attrs\n",
    "    how='intersection'\n",
    ")\n",
    "\n",
    "# 3. Compute overlap area for each pair (units: ftÂ² since EPSG:2263)\n",
    "intersections['overlap_area'] = intersections.geometry.area\n",
    "\n",
    "# 4. For each building (bbl), keep the row with the largest overlap_area\n",
    "idx_max = intersections.groupby('bbl')['overlap_area'].idxmax()\n",
    "largest_canopy_per_bldg = intersections.loc[idx_max].copy()\n",
    "\n",
    "largest_canopy_per_bldg = largest_canopy_per_bldg.rename(\n",
    "    columns={\"geometry\": \"canopy_geom\",\"Class\":\"canopy_change_class\"})\n",
    "\n",
    "buildings_with_largest_canopy = buildings_buffered.merge(\n",
    "    largest_canopy_per_bldg,\n",
    "    on='bbl',\n",
    "    how='left',\n",
    "    suffixes=('', '_canopy')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b08f0c-dc0d-4ee5-ba66-e6ccc15f105b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Taking a look\n",
    "buildings_with_largest_canopy.groupby(['canopy_change_class','year_ending_year']).agg({'bbl':\"nunique\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2f454-d577-44a4-b59a-9008dd6f22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limited ll84_enriched_ct_canopy data for joining with the energy metrics\n",
    "# ll84_enriched_ct_canopy.columns\n",
    "ll84_enriched_ct_canopy[ll84_enriched_ct_canopy[\"borough\"]==\"STATEN IS\"]=\"STATEN ISLAND\"\n",
    "\n",
    "## Striping those that are iffy, and keeping the solid ones. If needed i will clean later. \n",
    "print(ll84_enriched_ct_canopy.shape)\n",
    "ll84_enriched_ct_canopy_limited = ll84_enriched_ct_canopy[(\n",
    "    (ll84_enriched_ct_canopy[\"borough\"]==ll84_enriched_ct_canopy[\"borough_clean\"])\n",
    "    &(## manhattan\n",
    "        ((ll84_enriched_ct_canopy[\"bbl\"].astype(str).str[0] == \"1\") & (ll84_enriched_ct_canopy[\"borough\"]==\"MANHATTAN\"))\n",
    "    ##Bronx\n",
    "        |((ll84_enriched_ct_canopy[\"bbl\"].astype(str).str[0] == \"2\") & (ll84_enriched_ct_canopy[\"borough\"]==\"BRONX\"))\n",
    "    #Brooklyn\n",
    "         |((ll84_enriched_ct_canopy[\"bbl\"].astype(str).str[0] == \"3\") & (ll84_enriched_ct_canopy[\"borough\"]=='BROOKLYN'))\n",
    "    #Queens\n",
    "        |((ll84_enriched_ct_canopy[\"bbl\"].astype(str).str[0] == \"4\") & (ll84_enriched_ct_canopy[\"borough\"]==\"QUEENS\"))\n",
    "    #Staten Island\n",
    "    |((ll84_enriched_ct_canopy[\"bbl\"].astype(str).str[0] == \"5\") & (ll84_enriched_ct_canopy[\"borough\"]==\"STATEN ISLAND\"))\n",
    "    # Null BBL\n",
    "    |(ll84_enriched_ct_canopy[\"bbl\"].isnull())\n",
    "    ))]\n",
    "print(ll84_enriched_ct_canopy_limited.shape)\n",
    "### Dropped 1518 rows / buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ll84_enriched_ct_canopy.to_csv(\"ll84_enriched_ct_canopy.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2465e0b-4f50-44a9-bf58-ad4c53b7d5f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Below is manual work for column alignment for multi year data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7632befb-e853-4b96-8088-a70052934aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### \n",
    "column_dict = {}\n",
    "for year in residential3_2010_2017_lim4['year_ending_year'].unique():\n",
    "    print(year)\n",
    "    temp_df = residential3_2010_2017_lim4[residential3_2010_2017_lim4['year_ending_year']==year].dropna(how='all',axis=1)\n",
    "    print(list(temp_df.columns))\n",
    "    column_dict[year]=list(temp_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d5732-24f7-460e-81f5-f70541d72c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2010\n",
    "## Core for joining \n",
    "'year_ending_year',\n",
    "'property_id',\n",
    "'bbl',\n",
    "'address_1',\n",
    "'city',\n",
    "\"borough\",\n",
    "\"postcode\",\n",
    "'address_2',\n",
    "\"state\",\n",
    "## Data Information\n",
    "'release_date',\n",
    "\"last_modified_date\",\n",
    "\"current_energy_period_ending\",\n",
    "\"period_ending_date\",\n",
    "#Building\n",
    "\"total_floor_space_sq_ft\",\n",
    "## Energy Use\n",
    "'current_energy_period_ending', \n",
    "'current_site_energy_intensity', \n",
    "'current_total_site_energy', \n",
    "'target_site_energy_intensity',\n",
    "'current_source_energy', \n",
    "'current_total_source_energy', \n",
    "'current_weather_normalized', \n",
    "'total_indoor_and_outdoor', \n",
    "'current_total_on_site'\n",
    "#---------------------------------------\n",
    "## 2011\n",
    "## Core for joining \n",
    "'year_ending_year',\n",
    "'property_id',\n",
    "'bbl',\n",
    "'address_1',\n",
    "'city',\n",
    "\"borough\",\n",
    "\"postcode\",\n",
    "'address_2',\n",
    "\"state\",\n",
    "## Data Information\n",
    "\"release_date\"\n",
    "\"current_energy_period_ending\"\n",
    "\n",
    "## building\n",
    "\"total_floor_space_sq_ft\"\n",
    "\"year_built\"\n",
    "\"last_modified_date\"\n",
    "# metering_configuration  ?\n",
    "\n",
    "##energy\n",
    "weather_normalized_site_eui\n",
    "'current_energy_period_ending',\n",
    "'current_site_energy_intensity',\n",
    "'current_total_site_energy',\n",
    "'target_site_energy_intensity',\n",
    "'current_source_energy',\n",
    "'current_total_source_energy',\n",
    "'current_weather_normalized'\n",
    "###-------------------------\n",
    "##2012\n",
    "'year_ending_year'\n",
    "'property_id', \n",
    "'bbl'\n",
    " 'address_1',\n",
    "'city', \n",
    " 'borough',\n",
    " 'address_2',\n",
    " 'state_province'\n",
    "## Data info\n",
    "'generation_date'\n",
    "'release_date', \n",
    "'energy_current_date'\n",
    "#Building\n",
    "'year_built',\n",
    "'date_property_last_modified'\n",
    " 'property_floor_area_buildings'\n",
    "\n",
    "# Engergy\n",
    "'site_eui_kbtu_ft',\n",
    "'weather_normalized_site_eui',\n",
    "'site_energy_use_kbtu',\n",
    "'source_eui_kbtu_ft',\n",
    "'weather_normalized_source',\n",
    "'source_energy_use_kbtu',  \n",
    "'target_site_eui_kbtu_ft',\n",
    "### -------------------------\n",
    "### 2013\n",
    "## Core for joining\n",
    " 'year_ending_year',\n",
    "'property_id', \n",
    " 'bbl', \n",
    "'address_1', \n",
    "'city', \n",
    " 'borough',\n",
    "'postcode',  \n",
    "'address_2',\n",
    "## data info \n",
    "'generation_date',\n",
    "'release_date',\n",
    "\n",
    "#building info\n",
    "'year_built',  \n",
    " 'date_property_last_modified'\n",
    "'property_floor_area_building'\n",
    "#energy\n",
    "'site_eui_kbtu_ft',\n",
    "'weather_normalized_site_eui',\n",
    "'site_energy_use_kbtu',\n",
    "'weather_normalized_site_energy',\n",
    "'source_eui_kbtu_ft',\n",
    "'weather_normalized_source',\n",
    "'source_energy_use_kbtu',\n",
    "'weather_normalized_source_1', \n",
    "'weather_normalized_site',\n",
    "'weather_normalized_site_1',\n",
    "  'costar_property_id', \n",
    "###-------------------------------\n",
    "## 2014\n",
    "## Core for joining \n",
    "'year_ending_year',\n",
    "'property_id',\n",
    "'bbl',\n",
    "  'address_1', \n",
    "'city',\n",
    "'borough',\n",
    "'postcode'\n",
    "'address_2'\n",
    "## Data Information\n",
    "'generation_date',\n",
    "'release_date'\n",
    "\n",
    "## Buidling Info\n",
    "'year_built',\n",
    "'property_floor_area_building',\n",
    "## Energy Use\n",
    " 'site_eui_kbtu_ft', \n",
    "'weather_normalized_site_eui', \n",
    "'site_energy_use_kbtu', \n",
    "'weather_normalized_site_energy',\n",
    "'source_eui_kbtu_ft',\n",
    "'weather_normalized_source'\n",
    "'source_energy_use_kbtu'\n",
    " 'weather_normalized_source_1'\n",
    "'weather_normalized_site',\n",
    "'weather_normalized_site_1',\n",
    "'weather_normalized_site_2',\n",
    "'weather_normalized_site_3',\n",
    "## 2015\n",
    "'property_id',\n",
    "'address_1', \n",
    "'city',\n",
    " 'borough',\n",
    " 'postcode'\n",
    " 'address_2', \n",
    "## Core for joining \n",
    "\n",
    "## Data Information\n",
    "'generation_date'\n",
    "'release_date',\n",
    "## Buidling Info\n",
    "'largest_property_use_type_1',\n",
    "'year_built',\n",
    "'date_property_last_modified',\n",
    "\"property_gfa_self_reported\"\n",
    " 'property_gfa_epa_calculated', \n",
    "'property_gfa_epa_calculated_1'\n",
    "## Energy Use\n",
    "'site_eui_kbtu_ft',\n",
    "'weather_normalized_site_eui',\n",
    "'site_energy_use_kbtu',\n",
    "'weather_normalized_site_energy',\n",
    "'source_eui_kbtu_ft',\n",
    "'weather_normalized_source',\n",
    "'source_energy_use_kbtu',\n",
    "'weather_normalized_source_1', \n",
    " 'weather_normalized_site', 'weather_normalized_site_1', 'weather_normalized_site_2', 'weather_normalized_site_3',\n",
    "#-----------------------------\n",
    "## 2016\n",
    "## Core for joining \n",
    "'property_id',\n",
    "'bbl',\n",
    "'address_1',\n",
    "'city',\n",
    "'borough'\n",
    " 'postcode', \n",
    "'address_2',\n",
    "\n",
    "\n",
    "## Data Information\n",
    "'generation_date'\n",
    "'release_date',\n",
    "## Buidling Info\n",
    "'largest_property_use_type_1',\n",
    "'year_built',\n",
    " 'property_gfa_self_reported', \n",
    "'property_gfa_calculated', \n",
    "'property_gfa_calculated_1',\n",
    "'date_property_last_modified'\n",
    "## Energy Use\n",
    "'site_eui_kbtu_ft',\n",
    "'weather_normalized_site_eui',\n",
    "'site_energy_use_kbtu',\n",
    "'weather_normalized_site_energy',\n",
    "'source_eui_kbtu_ft',\n",
    "'weather_normalized_source',\n",
    "'source_energy_use_kbtu',\n",
    "'weather_normalized_source_1',\n",
    " 'weather_normalized_site', \n",
    "'weather_normalized_site_1', \n",
    "'weather_normalized_site_2', \n",
    "'weather_normalized_site_3',\n",
    "'source_energy_use_adjusted'\n",
    "'source_eui_adjusted_to_current'\n",
    "##### --------------------------------------\n",
    "## 2017\n",
    "## Core for joining\n",
    " 'year_ending_year'\n",
    "'property_id', \n",
    "'bbl'\n",
    "'address_1',\n",
    "'city',  \n",
    "'borough',\n",
    "'postcode', \n",
    "'address_2',  \\\n",
    "\n",
    "## Data Information\n",
    "'generation_date'\n",
    "\n",
    "## Buidling Info\n",
    " 'largest_property_use_type_1', \n",
    "'year_built',\n",
    "'property_gfa_self_reported',\n",
    "'property_gfa_calculated', \n",
    "'property_gfa_calculated_1\n",
    "'last_modified_date_property',\n",
    "## Energy Use\n",
    "'site_eui_kbtu_ft', \n",
    "'weather_normalized_site_eui', \n",
    "'site_energy_use_kbtu',\n",
    "'weather_normalized_site_energy',\n",
    "'source_eui_kbtu_ft',\n",
    "'weather_normalized_source',\n",
    "'source_energy_use_kbtu',\n",
    "'weather_normalized_source_1', \n",
    " 'weather_normalized_site', \n",
    "'weather_normalized_site_1',\n",
    "'weather_normalized_site_2',\n",
    "'weather_normalized_site_3', \n",
    " 'source_energy_use_adjusted',\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
