{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf0c00-19bd-45ae-8dc3-0959b0a0bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# ---- NYC Open Data dataset IDs ----\n",
    "DS_TREE_POINTS = \"hn5i-inap\"  # Forestry Tree Points (street-tree planting spaces / trees)\n",
    "DS_WORK_ORDERS = \"bdjm-n7q4\"  # Forestry Work Orders (events: removals, plantings, etc.)\n",
    "DS_CENSUS_2015 = \"uvpi-gqnh\"  #  2015 Street Tree Census - dont knwo if i ineed\n",
    "\n",
    "# ---- Analysis window ----\n",
    "YEARS = list(range(2010, 2018))  # 2010..2017 inclusive\n",
    "AS_OF = {y: pd.Timestamp(f\"{y}-12-31\") for y in YEARS}\n",
    "\n",
    "# Spatial CRS for NYC (planar feet) if you do spatial joins later\n",
    "TARGET_CRS = \"EPSG:2263\"\n",
    "BASE = \"https://data.cityofnewyork.us/resource/{id}\"\n",
    "\n",
    "### Setting Pandas Options \n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a899e9-2e6f-4b1c-9264-4a44d4bce211",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining functions in roder to pull the data as needed. \n",
    "def _headers():\n",
    "    h = {}\n",
    "    tok = os.getenv(\"SOCRATA_APP_TOKEN\")\n",
    "    if tok:\n",
    "        h[\"X-App-Token\"] = tok\n",
    "    return h\n",
    "\n",
    "def socrata_fetch_tabular(dataset_id, where=None, select=None, limit=25000, max_rows=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Pull ALL rows (paged) from a Socrata tabular dataset (.json). Returns a DataFrame.\n",
    "    Set max_rows=None to attempt full download.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE.format(id=dataset_id)}.json\"\n",
    "    params_base = {\"$limit\": limit}\n",
    "    if where:  params_base[\"$where\"]  = where\n",
    "    if select: params_base[\"$select\"] = select\n",
    "\n",
    "    frames, offset, got = [], 0, 0\n",
    "    while True:\n",
    "        if max_rows is not None and got >= max_rows:\n",
    "            break\n",
    "        page = min(limit, max_rows - got) if max_rows else limit\n",
    "        p = dict(params_base); p[\"$offset\"] = offset; p[\"$limit\"] = page\n",
    "        r = requests.get(url, params=p, headers=_headers(), timeout=180); r.raise_for_status()\n",
    "        rows = r.json()\n",
    "        if not rows: break\n",
    "        frames.append(pd.DataFrame(rows))\n",
    "        n = len(rows); got += n; offset += n\n",
    "        if verbose: print(f\"{dataset_id}: fetched {got:,} rows…\")\n",
    "        if n < page: break\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def socrata_fetch_geojson(dataset_id, limit=50000, verbose=True):\n",
    "    \"\"\"\n",
    "    Pull ALL features from a Socrata GeoJSON endpoint (.geojson). Returns a GeoDataFrame (EPSG:4326).\n",
    "    Useful for polygon layers like census tracts.\n",
    "    \"\"\"\n",
    "    feats, offset = [], 0\n",
    "    while True:\n",
    "        url = f\"{BASE.format(id=dataset_id)}.geojson?$limit={limit}&$offset={offset}\"\n",
    "        r = requests.get(url, headers=_headers(), timeout=180); r.raise_for_status()\n",
    "        gj = r.json(); chunk = gj.get(\"features\", [])\n",
    "        if not chunk: break\n",
    "        feats.extend(chunk); offset += len(chunk)\n",
    "        if verbose: print(f\"{dataset_id} (geojson): fetched {offset:,} features…\")\n",
    "        if len(chunk) < limit: break\n",
    "    return gpd.GeoDataFrame.from_features(feats, crs=\"EPSG:4326\") if feats else gpd.GeoDataFrame(geometry=[], crs=\"EPSG:4326\")\n",
    "\n",
    "def add_lat_lon_from_location(df, location_col=\"location\"):\n",
    "    \"\"\"\n",
    "    Unpack Socrata 'location' Point dict into numeric lat/lon, preserving all other columns.\n",
    "    \"\"\"\n",
    "    if location_col not in df.columns: return df\n",
    "    def _lat(v):\n",
    "        if isinstance(v, dict) and \"latitude\" in v: return v[\"latitude\"]\n",
    "        if isinstance(v, dict) and \"coordinates\" in v: return v[\"coordinates\"][1]\n",
    "    def _lon(v):\n",
    "        if isinstance(v, dict) and \"longitude\" in v: return v[\"longitude\"]\n",
    "        if isinstance(v, dict) and \"coordinates\" in v: return v[\"coordinates\"][0]\n",
    "    out = df.copy()\n",
    "    out[\"lat\"] = pd.to_numeric(out[location_col].map(_lat), errors=\"coerce\")\n",
    "    out[\"lon\"] = pd.to_numeric(out[location_col].map(_lon), errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def convert_datetime_columns(df, extra_force=()):\n",
    "    \"\"\"\n",
    "    Convert columns that look like dates/times (by name) to pandas datetime.\n",
    "    extra_force: iterable of column names to always attempt.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    name_pat = re.compile(r\"(date|time|_at|_on)\", re.I)\n",
    "    candidates = [c for c in df.columns if name_pat.search(c)]\n",
    "    candidates = sorted(set(candidates).union(extra_force))\n",
    "    for c in candidates:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def classify_work_orders(works):\n",
    "    \"\"\"\n",
    "    Add boolean flags for 'is_tree_removal' and 'is_tree_planting' + pick a best 'event_date'.\n",
    "    Uses wotype + wstatus. Tighten patterns later if you discover coded fields.\n",
    "    \"\"\"\n",
    "    w = works.copy()\n",
    "    def s(x): return x.astype(str).str.lower() if isinstance(x, pd.Series) else pd.Series([], dtype=str)\n",
    "    wotype, wstatus = s(w.get(\"wotype\")), s(w.get(\"wstatus\"))\n",
    "\n",
    "    is_closed   = wstatus.isin({\"closed\",\"complete\",\"completed\"})\n",
    "    is_removal  = wotype.str.contains(r\"\\btree removal\\b\", na=False) | \\\n",
    "                  wotype.str.contains(r\"\\btree removal for tree planting\\b\", na=False) | \\\n",
    "                  wotype.str.contains(r\"\\btree down\\b\", na=False)\n",
    "    is_planting = wotype.str.contains(r\"\\btree plant\\b\", na=False) | \\\n",
    "                  wotype.str.contains(r\"\\bplanting\\b\", na=False)\n",
    "\n",
    "    w[\"is_tree_removal\"]  = is_removal & is_closed\n",
    "    w[\"is_tree_planting\"] = is_planting & is_closed\n",
    "\n",
    "    # Choose a best event date per row: prefer completion/closed → else earliest parsed date on row\n",
    "    date_priority = [c for c in [\n",
    "        \"completeddate\",\"closeddate\",\"finishdate\",\"completion_date\",\"date_completed\",\n",
    "        \"actual_end_date\",\"actual_finish_date\"\n",
    "    ] if c in w.columns]\n",
    "    dt_cols = [c for c in w.columns if pd.api.types.is_datetime64_any_dtype(w[c])]\n",
    "\n",
    "    def pick_event_date(row):\n",
    "        for c in date_priority:\n",
    "            v = row.get(c)\n",
    "            if pd.notna(v): return v\n",
    "        vals = [row[c] for c in dt_cols if pd.notna(row[c])]\n",
    "        return min(vals) if vals else pd.NaT\n",
    "\n",
    "    w[\"event_date\"] = w.apply(pick_event_date, axis=1)\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50edf1ef-e78c-49f3-8bc3-790bc7d69150",
   "metadata": {},
   "source": [
    "####  Pulling in the Data from the NYC OPEN DATA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720f153-4202-4866-bab3-f4a71e82a66e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Forestry Tree Points (all columns; keep only rows with geometry) ---\n",
    "points_raw = socrata_fetch_tabular(\n",
    "    DS_TREE_POINTS,\n",
    "    where=\"location IS NOT NULL\",\n",
    "    select=None,\n",
    "    limit=25000,\n",
    "    max_rows=None\n",
    ")\n",
    "points = add_lat_lon_from_location(points_raw, \"location\").dropna(subset=[\"lat\",\"lon\"])\n",
    "points = convert_datetime_columns(points, extra_force=(\"createddate\", \"updateddate\"))\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# Use 'createddate' as baseline start; we can refine with planting WOs later if earlier\n",
    "if \"createddate\" not in points.columns:\n",
    "    raise KeyError(\"Expected 'createddate' in Tree Points. Show points.columns if it's named differently.\")\n",
    "\n",
    "# Planting-space key in Tree Points\n",
    "key_points = \"plantingspaceglobalid\" if \"plantingspaceglobalid\" in points.columns else \\\n",
    "             \"planting_space_global_id\" if \"planting_space_global_id\" in points.columns else None\n",
    "if not key_points:\n",
    "    raise KeyError(\"Expected a planting space id in Tree Points (e.g., 'plantingspaceglobalid').\")\n",
    "\n",
    "# --- Forestry Work Orders (all columns) ---\n",
    "works_raw = socrata_fetch_tabular(\n",
    "    DS_WORK_ORDERS,\n",
    "    where=None,\n",
    "    select=None,\n",
    "    limit=25000,\n",
    "    max_rows=None\n",
    ")\n",
    "works = convert_datetime_columns(works_raw)\n",
    "works = classify_work_orders(works)\n",
    "\n",
    "# Planting-space key in Work Orders\n",
    "key_works = \"plantingspaceglobalid\" if \"plantingspaceglobalid\" in works.columns else \\\n",
    "            \"planting_space_global_id\" if \"planting_space_global_id\" in works.columns else None\n",
    "if not key_works:\n",
    "    print(\"⚠️ No planting-space key found in Work Orders; one-to-many join will be skipped.\")\n",
    "\n",
    "# --- 2015 Tree Census for enrichment (species, DBH cross-check, etc.) ---\n",
    "# You can use this later for QA or to backfill attributes where Tree Points are missing.\n",
    "census15 = socrata_fetch_tabular(\n",
    "    DS_CENSUS_2015,\n",
    "    where=\"latitude IS NOT NULL AND longitude IS NOT NULL\",\n",
    "    select=None,\n",
    "    limit=25000,\n",
    "    max_rows=200_000  # start smaller; set None to attempt full\n",
    ")\n",
    "census15 = convert_datetime_columns(census15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84933b53-4639-4ab4-a6ba-254ff97b1dc6",
   "metadata": {},
   "source": [
    "#### Taking the Forestry Tree points data and limiting to the window of interest (2010 - 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f7996-6764-4659-983c-3ccb700999fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_created_max = pd.Timestamp(\"2017-12-31\")   # keep created <= this (exclude NaT and > 2017-12-31)\n",
    "cut_planted_max = pd.Timestamp(\"2009-12-31\")   # accept planted <= this OR NaT\n",
    "\n",
    "### Assuptions made: Null values would imply older trees, while Newly planted trees are probably too. So essentially if they were planted in the window\n",
    "# We ignore.\n",
    "\n",
    "# --- Masks per your rules ---\n",
    "# 1) CREATEDDATE: keep only rows with a non-null createddate on/before 2017-12-31\n",
    "mask_created = points[\"createddate\"].isna() | (points[\"createddate\"] <= cut_created_max)\n",
    "\n",
    "# 2) PLANTEDDATE: keep rows where planteddate is NaT OR planteddate <= 2009-12-31 We want trees planted beofre the window.\n",
    "## Newly planted trees are probably too \n",
    "mask_planted = points[\"planteddate\"].isna() | (points[\"planteddate\"] <= cut_planted_max)\n",
    "\n",
    "# --- Apply both filters ---\n",
    "points_filtered = points.loc[mask_created & mask_planted].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afc263-1d82-4838-a816-d62c55278a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only Alive Trees\n",
    "points_filter = points_filtered[points_filtered['tpcondition']!='Dead']\n",
    "points_filter = points_filter[~points_filter['tpstructure'].isin(['Stump - Uprooted','Stump'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190397a2-ebf6-4251-a803-005c9ded5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_filter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfd4de-0d38-4224-8e33-7b05668566ef",
   "metadata": {},
   "source": [
    "#### Starting with Work Order data for trees\n",
    "- Limiting to those within the window, Not open, so closed (action taken), and using the data to find trees removed during the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d4792-ccd0-49a4-887e-e838bee1879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Pick the right status column and filter to CLOSED work orders\n",
    "status_col = \"wostatus\" if \"wostatus\" in works_raw.columns else \"wstatus\"\n",
    "closed_orders = works_raw.loc[\n",
    "    works_raw[status_col].astype(str).str.lower().eq(\"closed\")\n",
    "].copy()\n",
    "\n",
    "# 2) Parse dates\n",
    "closed_orders[\"createddate\"] = pd.to_datetime(closed_orders[\"createddate\"], errors=\"coerce\")\n",
    "closed_orders[\"createddate\"] = pd.to_datetime(closed_orders[\"createddate\"], errors=\"coerce\")\n",
    "closed_orders[\"closeddate\"] = pd.to_datetime(closed_orders[\"closeddate\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# 3) Keep rows with createddate < 2018-01-01 OR createddate is NaT\n",
    "cut_off = pd.Timestamp(\"2018-01-01\")\n",
    "mask = ((closed_orders[\"createddate\"].isna()) | (closed_orders[\"createddate\"] < cut_off)\n",
    "       &((closed_orders[\"createddate\"].isna()) | (closed_orders[\"createddate\"] < cut_off))\n",
    "       &(closed_orders[\"closeddate\"].isna() | (closed_orders[\"closeddate\"] < cut_off)))\n",
    "closed_orders_2017 = closed_orders.loc[mask].copy()\n",
    "closed_orders_2017\n",
    "\n",
    "print(len(closed_orders), \"closed orders total\")\n",
    "print(len(closed_orders_2017), \"closed orders with createddate < 2018-01-01 or NaT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57bdef2-b7c2-4714-9905-957687ca3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Looking at removals to flag via object ID in the forestry points data\n",
    "removals = closed_orders_2017[((closed_orders_2017[\"wotype\"].astype(str).str.lower().str.contains(\"removal\"))|\n",
    "    (closed_orders_2017[\"wocategory\"].astype(str).str.lower().str.contains(\"removal\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803d807-bce7-4141-8b95-4c8276837874",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving files\n",
    "# points_filter.to_csv(\"tree_points_filters.csv\",index=False)\n",
    "# removals.to_csv(\"tree_removals.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bcdcb-390b-4ff4-a9f2-77ab326e44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_removed = points_filter[~points_filter['objectid'].isin(removals['objectid'].unique())]\n",
    "print(non_removed.shape)\n",
    "removed = points_filter[points_filter['objectid'].isin(removals['objectid'].unique())]\n",
    "print(removed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfc270-a34b-4462-8142-324b3f567e6b",
   "metadata": {},
   "source": [
    "#### Confirming that there are no duplicate tree listings in the work orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8abe3-dc32-47d5-82ab-f456328dace3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removed_lim = removed[[\"objectid\",\"updateddate\",\"createddate\",\"riskratingdate\"]].drop_duplicates()\n",
    "#Checking object Id is 1:1 for removals\n",
    "test = removed_lim.groupby([\"objectid\"]).agg({\"updateddate\":\"count\",\"createddate\":\"count\"}).reset_index()\n",
    "test[\"updateddate\"][test[\"updateddate\"]==0]=np.nan\n",
    "test[\"entry_count\"] = test[\"updateddate\"].combine_first(test[\"createddate\"])\n",
    "test = test.drop(columns=[\"updateddate\",\"createddate\"])\n",
    "test[test[\"entry_count\"]>1]\n",
    "## COnfirm no dupes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b23e55-3e07-4532-b68a-7ad92599c388",
   "metadata": {},
   "source": [
    "#### Using Contextual Ticket dates to identify Est. Removal Date. \n",
    "- Only interested in those here that were removed WITHIN the analysis window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb2ce1-a6d1-48f2-9ef1-d39a38bbbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figuring out the Removed trees and when they were removed\n",
    "# 1) Make sure the dates are real datetimes\n",
    "for c in [\"updateddate\", \"createddate\"]:\n",
    "    removed[c] = pd.to_datetime(removed[c], errors=\"coerce\")\n",
    "\n",
    "# 2) Removal date estimate: prefer updateddate, fall back to createddate\n",
    "removed.loc[:, \"removal_date_est\"] = removed[\"updateddate\"].fillna(removed[\"createddate\"])\n",
    "cutoff = pd.Timestamp(\"2018-01-01\")  # everything strictly earlier than this\n",
    "removed.loc[:, \"removed_before_2018\"] = removed[\"removal_date_est\"] < cutoff\n",
    "\n",
    "# Optional: keep only those rows\n",
    "removed_2017_and_earlier = removed[removed[\"removed_before_2018\"]]\n",
    "\n",
    "## NO nulls, Checked\n",
    "# removed[removed[\"Removal_Data_Est\"].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f2cde-e2d4-4bbe-a8cc-e0985e2f7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(removed.shape)\n",
    "print(removed_2017_and_earlier.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a5552-0286-470d-8b5b-febe95519034",
   "metadata": {},
   "source": [
    "##### for those removals, adding flag to include or not incluide in analysis based on when during the eyar they were removed. \n",
    "- Those removed in June or before will not be included in year, but July and after included. Essentially if it was ardoun for majority of year - keep it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb4af1-ecf4-4ea3-85a6-000f3a8fc0bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removed_2017_and_earlier[\"removal_year\"] =  removed_2017_and_earlier[\"removal_date_est\"].dt.year\n",
    "# 2) Include-in-year flag:\n",
    "#    True  -> removed in July (7) or later\n",
    "#    False -> removed in June (6) or earlier\n",
    "#    NaT   -> treated as False (not included)\n",
    "m = removed_2017_and_earlier[\"removal_date_est\"].dt.month\n",
    "removed_2017_and_earlier[\"include_in_year\"] = (m >= 7) & m.notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c6057-a7eb-43a2-926b-60eba169c4d0",
   "metadata": {},
   "source": [
    "#### Performing the join with the filtered Points data to get usable tree existance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b44df-4344-43d5-86e8-10b2965805e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removed_lim_join = removed_2017_and_earlier[[\"objectid\",\"removal_date_est\",\"removed_before_2018\",\"removal_year\",\"include_in_year\"]]\n",
    "trees_raw = points_filter.merge(removed_lim_join, how='left',on = [\"objectid\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d69f59-30cd-4a53-901a-cf2f6b929d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##*** Analysis assumption is that the trees from this data set were around in 2010. ***\n",
    "trees_raw[\"removed_before_2018\"][trees_raw[\"removed_before_2018\"].isnull()]=False\n",
    "trees_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68729f-7dee-47f8-864b-15faf3310f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c5794-7711-45af-a6c0-2d06d200d366",
   "metadata": {},
   "source": [
    "### Flattening the data long ways for each year in the analysis, removing the trees based on found removal work orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f8f63-4dc7-42b3-ae98-d695e729ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Processing the Tree Data to Have Yearly values for eahc tree:\n",
    "trees_processing = []\n",
    "trees_raw['manual_year'] = None\n",
    "for year in range(2010,2018):\n",
    "    print(year)\n",
    "    trees_raw['manual_year'] = year\n",
    "    trees_incl = trees_raw[((trees_raw[\"removal_year\"].isnull())\n",
    "                        |(trees_raw[\"removal_year\"]>year)\n",
    "                        |((trees_raw[\"removal_year\"]==year) & (trees_raw[\"include_in_year\"]==True)))]\n",
    "    trees_processing.append(trees_incl)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ae413-bea3-48d3-b553-41ec4d2a85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_processed = pd.concat(trees_processing)\n",
    "# trees_processed.to_csv(\"trees_processed_for_part3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
